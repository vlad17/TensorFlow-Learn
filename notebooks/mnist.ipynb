{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This notebook somewhat follows the tutorial from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "#\n",
    "# The current working directory is expected to be $PROJECT_ROOT/notebooks\n",
    "\n",
    "import sys, os\n",
    "if '../code' not in sys.path: sys.path.append('../code')\n",
    "import mnist_downloader\n",
    "import numpy as np\n",
    "import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x            | y           |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "\n",
      "x float32 y float32\n"
     ]
    }
   ],
   "source": [
    "train, test = mnist_downloader.read_data_sets('../data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "from data_set import DataSet\n",
    "from tabulate import tabulate\n",
    "\n",
    "tbl = tabulate([['train', train.x.shape, train.y.shape],\n",
    "                ['test', test.x.shape, test.y.shape]],\n",
    "               ['Data Set', 'x', 'y'], tablefmt='grid')\n",
    "print('\\n{}\\n\\nx {!s} y {!s}'.format(tbl, train.x.dtype, train.y.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n",
      "  Epoch   5 error 0.08846664428710938\n",
      "  Epoch  10 error 0.087066650390625\n",
      "  Epoch  15 error 0.08719998598098755\n",
      "Starting fold 2\n",
      "  Epoch   5 error 0.08313333988189697\n",
      "  Epoch  10 error 0.08226668834686279\n",
      "  Epoch  15 error 0.08193331956863403\n",
      "  Epoch  20 error 0.08206665515899658\n",
      "Starting fold 3\n",
      "  Epoch   5 error 0.08853334188461304\n",
      "  Epoch  10 error 0.10439997911453247\n",
      "Starting fold 4\n",
      "  Epoch   5 error 0.0835999846458435\n",
      "  Epoch  10 error 0.08373332023620605\n",
      "Fold 2/4 epoch 15/100 with inside training dataset-error 0.07331669330596924\n",
      "Test error 0.0782999992371\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "sm = regression.SoftMax(x, y)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False) # don't train a global step value\n",
    "initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           decay_period, decay_fraction, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "             .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_FOLDS = 4\n",
    "UPDATE_COARSENESS = 5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "top_models = []\n",
    "\n",
    "# Cross validate on random restarts\n",
    "for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "    print('Starting fold {}'.format(fold))\n",
    "    sess.run(tf.initialize_all_variables()) # random restart\n",
    "    best_in_fold_validation = np.inf\n",
    "    best_in_fold_epoch = None\n",
    "    best_in_fold_model = None\n",
    "\n",
    "    for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "        for batch in train.new_epoch(BATCH_SIZE):\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1]}, session=sess)\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            valid_err = validate.multiclass_error(x, sm.y, y, session=sess)\n",
    "            print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "            if valid_err > best_in_fold_validation: break\n",
    "            best_in_fold_validation = valid_err\n",
    "            best_in_fold_epoch = epoch\n",
    "            best_in_fold_model = saver.save(sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "    top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "# We select the model based on the entire training data set rather than the error from the\n",
    "# cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "# compare across the folds the same accuracy since some folds might be easier than others.\n",
    "# Doing this helps the test accuracy by about 0.5%.\n",
    "best_fold, best_epoch, best_model = None, None, None\n",
    "best_error = np.inf\n",
    "for fold, epoch, model in top_models:\n",
    "    saver.restore(sess, model)\n",
    "    error = train.multiclass_error(x, sm.y, y, session=sess)\n",
    "    if best_error > error:\n",
    "        best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "\n",
    "print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "      .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "\n",
    "saver.restore(sess, best_model)\n",
    "\n",
    "print('Test error', test.multiclass_error(x, sm.y, y, session=sess))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mnist-cnn-35.ckpt']\n",
      "1 saved epoch files; using largest epoch (35) as start\n"
     ]
    }
   ],
   "source": [
    "cnn_prefix = 'mnist-cnn-'\n",
    "cnn_suffix = '.ckpt'\n",
    "available_epochs = [x for x in os.listdir('../data')\n",
    "                    if x.startswith(cnn_prefix) and x.endswith(cnn_suffix)]\n",
    "print(available_epochs)\n",
    "\n",
    "def extract_epoch_number(s):\n",
    "    return int(s[len(cnn_prefix):-len(cnn_suffix)])\n",
    "\n",
    "available_epochs = [extract_epoch_number(s) for s in available_epochs]\n",
    "max_epoch = max(available_epochs or [None])\n",
    "print('{} saved epoch files; using largest epoch ({}) as start'\n",
    "      .format(len(available_epochs), max_epoch))\n",
    "\n",
    "def epoch_to_filename(e):\n",
    "    return '../data/' + cnn_prefix + str(e) + cnn_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Variable:0\", shape=(5, 5, 1, 32), dtype=float32_ref) must be from the same graph as Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-23fcede7c26b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mW_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mb_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbias_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mh_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_conv1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mh_pool1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool_2x2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-23fcede7c26b>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, W)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmax_pool_2x2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    293\u001b[0m                               \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                               \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                               data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[1;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m       \u001b[1;31m# pyline: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   3497\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3498\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3499\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3500\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3501\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   3442\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m     raise ValueError(\n\u001b[1;32m-> 3444\u001b[1;33m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[0;32m   3445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"Variable:0\", shape=(5, 5, 1, 32), dtype=float32_ref) must be from the same graph as Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Don't drop out when testing by setting keep_prob to 1.0\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if max_epoch:\n",
    "    saver.restore(sess, epoch_to_filename(max_epoch))\n",
    "    print('Restoring CNN from epoch {}'.format(max_epoch))\n",
    "else:\n",
    "    print('Starting CNN training from scratch')\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    max_epoch = 0\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 35\n",
    "UPDATE_COARSENESS = 5\n",
    "for epoch in range(max_epoch + 1, NUM_EPOCHS + 1):\n",
    "    for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "        tot_batches = train.size // BATCH_SIZE\n",
    "        two_percent_done = i * 50 // tot_batches\n",
    "        print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "               + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5}, session=sess)\n",
    "\n",
    "    print(']')\n",
    "    name = saver.save(sess, epoch_to_filename(epoch))\n",
    "    print('  Saved to', name)\n",
    "    if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "        err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)\n",
    "        print('  Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "\n",
    "print('Test error {}'.format(test.multiclass_error(\n",
    "            x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
