{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# This notebook somewhat follows the tutorials from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "if '../code' not in sys.path:\n",
    "    sys.path.append('../code')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport mnist_downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x float32    | y float32   |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "trainx, trainy, testx, testy = mnist_downloader.read_data_sets(\n",
    "    '../data', one_hot=True, exact_inputs=False)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(\n",
    "    [['train', trainx.shape, trainy.shape],\n",
    "     ['test', testx.shape, testy.shape]],\n",
    "    ['Data Set', f'x {trainx.dtype!s}', f'y {trainy.dtype!s}'],\n",
    "    tablefmt='grid'))\n",
    "\n",
    "# TODO check mnist balance; autobalance (look up techniques? papers, how does TF-slim do it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca55ba4a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify dataset balance\n",
    "\n",
    "plt.hist(trainy.argmax(1), 10, facecolor='green', alpha=0.75)\n",
    "plt.axhline(len(trainy) / 10, color='red', ls=':')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('counts')\n",
    "plt.title(r'histogram of training labels')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_and_validx = trainx\n",
    "train_and_validy = trainy\n",
    "trainx, validx, trainy, validy = train_test_split(\n",
    "    train_and_validx, train_and_validy, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def evaluate(truey_onehot, predy_onehot):\n",
    "    truey = truey_onehot.argmax(1)\n",
    "    predy = predy_onehot.argmax(1)\n",
    "    \n",
    "    nclasses = 10\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(truey, predy).astype(float)\n",
    "    np.fill_diagonal(confusion, np.nan)\n",
    "    plt.imshow(confusion)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(nclasses))\n",
    "    plt.yticks(np.arange(nclasses))\n",
    "    plt.xlabel(r'$\\hat{y}$')\n",
    "    plt.ylabel(r'$y$')\n",
    "    plt.title(r'confusion counts ($n={}K$)'.format(len(validy)//1000))\n",
    "    plt.show()\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(truey, predy)\n",
    "    rocauc = sklearn.metrics.roc_auc_score(truey_onehot, predy_onehot, average='weighted')\n",
    "\n",
    "    print(tabulate([[rocauc, acc]], ['weighted roc auc', 'accuracy'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Reshape, Conv2D, MaxPooling2D, Maximum, LocallyConnected2D\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Reshape, Concatenate\n",
    "\n",
    "def generate_network(name):\n",
    "    if name == 'google':\n",
    "        return Sequential([\n",
    "    Reshape([28, 28, 1], input_shape=(trainx.shape[1],)),\n",
    "    Conv2D(32, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    Conv2D(64, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    Reshape([28 * 28 * 64 // 4 // 4]),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "    elif name == 'mlp-6':\n",
    "        return Sequential([\n",
    "    Dense(600, activation='relu', input_dim=trainx.shape[1]),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "    elif name == 'lc':\n",
    "        return Sequential([\n",
    "        Reshape([28, 28, 1], input_shape=(trainx.shape[1],)),\n",
    "        Conv2D(64, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "        LocallyConnected2D(64, [5, 5], strides=(1, 1), padding='valid', activation='relu'),\n",
    "        Reshape([(28 // 2 - 5 + 1) ** 2 * 64]),\n",
    "        Dropout(0.8),            \n",
    "    Dense(10, activation='softmax')\n",
    "        ])\n",
    "    elif name == 'logistic':\n",
    "        return Sequential(\n",
    "        [Dense(10, input_dim=trainx.shape[1]),\n",
    "     Activation('softmax')])\n",
    "    else:\n",
    "        raise ValueError('Invalid net name ' + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** google *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 7s - loss: 0.1876 - acc: 0.9423 - val_loss: 0.0549 - val_acc: 0.9827\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 7s - loss: 0.0580 - acc: 0.9828 - val_loss: 0.0359 - val_acc: 0.9892\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 7s - loss: 0.0370 - acc: 0.9885 - val_loss: 0.0309 - val_acc: 0.9895\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 7s - loss: 0.0285 - acc: 0.9908 - val_loss: 0.0339 - val_acc: 0.9888\n",
      "  30 sec\n",
      "***** mlp-6 *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s - loss: 0.2687 - acc: 0.9191 - val_loss: 0.1332 - val_acc: 0.9582\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s - loss: 0.1207 - acc: 0.9636 - val_loss: 0.1263 - val_acc: 0.9617\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s - loss: 0.0823 - acc: 0.9757 - val_loss: 0.1039 - val_acc: 0.9706\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 2s - loss: 0.0535 - acc: 0.9841 - val_loss: 0.1113 - val_acc: 0.9716\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 2s - loss: 0.0506 - acc: 0.9850 - val_loss: 0.1238 - val_acc: 0.9668\n",
      "  16 sec\n",
      "***** lc *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.2559 - acc: 0.9228 - val_loss: 0.0707 - val_acc: 0.9783\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0881 - acc: 0.9728 - val_loss: 0.0505 - val_acc: 0.9851\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0661 - acc: 0.9795 - val_loss: 0.0465 - val_acc: 0.9852\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0540 - acc: 0.9827 - val_loss: 0.0388 - val_acc: 0.9875\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0477 - acc: 0.9850 - val_loss: 0.0363 - val_acc: 0.9885\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0408 - acc: 0.9867 - val_loss: 0.0327 - val_acc: 0.9893\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0356 - acc: 0.9885 - val_loss: 0.0305 - val_acc: 0.9901\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0312 - acc: 0.9894 - val_loss: 0.0393 - val_acc: 0.9883\n",
      " 134 sec\n",
      "***** logistic *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.7788 - acc: 0.8106 - val_loss: 0.4529 - val_acc: 0.8848\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.4006 - acc: 0.8952 - val_loss: 0.3647 - val_acc: 0.9010\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.3445 - acc: 0.9068 - val_loss: 0.3316 - val_acc: 0.9083\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.3185 - acc: 0.9136 - val_loss: 0.3143 - val_acc: 0.9118\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.3027 - acc: 0.9178 - val_loss: 0.3059 - val_acc: 0.9126\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.2920 - acc: 0.9192 - val_loss: 0.2972 - val_acc: 0.9153\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.2840 - acc: 0.9221 - val_loss: 0.2922 - val_acc: 0.9172\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.2777 - acc: 0.9234 - val_loss: 0.2896 - val_acc: 0.9186\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s - loss: 0.2727 - acc: 0.9245 - val_loss: 0.2855 - val_acc: 0.9183\n",
      "  11 sec\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ../models && mkdir -p ../models/mnist\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from keras import backend as K\n",
    "\n",
    "@contextmanager\n",
    "def rectime(name='', fmt='{: 4.0f}'):\n",
    "    print(name, end='')\n",
    "    sys.stdout.flush()\n",
    "    t = time.time()\n",
    "    yield\n",
    "    t = time.time() - t\n",
    "    print(fmt.format(t), 'sec')\n",
    "    \n",
    "names = ['google', 'mlp-6', 'lc', 'logistic'] # convert all to dictionary above so .keys() here\n",
    "save_files = {name: '../models/mnist/' + name + '-best.hdf5' for name in names}\n",
    "for name in names:\n",
    "    print('*****', name, '*****')\n",
    "\n",
    "    K.clear_session()\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    net = generate_network(name)\n",
    "    net.compile(\n",
    "        loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc', min_delta=0, patience=0)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(save_files[name], monitor='val_loss', save_best_only=True)\n",
    "    # TODO more templates to have: TensorBoard on aws; filters (logistic?)\n",
    "    # feeder queues\n",
    "    # norms over gradients (tensorboard example?)\n",
    "    # widgets investigate\n",
    "    # investigate model fits on gpu, but minibatch doesn't (need to split it up b/w devices?)\n",
    "    # -> https://github.com/tensorflow/models/tree/master/inception#how-to-fine-tune-a-pre-trained-model-on-a-new-task\n",
    "    # compress this routine and the one below. Move nets to dictionaries?\n",
    "    # history recorder\n",
    "    with rectime():\n",
    "        net.fit(\n",
    "            trainx,\n",
    "            trainy,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "            validation_data=(validx, validy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** google (post-train) *****\n",
      "training until loss <= 0.0180\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 9s - loss: 0.0358 - acc: 0.9890     \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 9s - loss: 0.0249 - acc: 0.9923     \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 9s - loss: 0.0193 - acc: 0.9938     \n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 9s - loss: 0.0141 - acc: 0.9955     \n",
      "  39 sec\n",
      "***** mlp-6 (post-train) *****\n",
      "training until loss <= 0.0576\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s - loss: 0.0862 - acc: 0.9767     \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 3s - loss: 0.0612 - acc: 0.9828     \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 3s - loss: 0.0433 - acc: 0.9877     \n",
      "  12 sec\n",
      "***** lc (post-train) *****\n",
      "training until loss <= 0.0124\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0399 - acc: 0.9872    \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0341 - acc: 0.9893    \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0299 - acc: 0.9906    \n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0260 - acc: 0.9916    \n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0233 - acc: 0.9924    \n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0221 - acc: 0.9930    \n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0185 - acc: 0.9943    \n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0183 - acc: 0.9939    \n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0163 - acc: 0.9947    \n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0159 - acc: 0.9948    \n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0149 - acc: 0.9951    \n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0130 - acc: 0.9957    \n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0126 - acc: 0.9961    \n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0125 - acc: 0.9959    \n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0106 - acc: 0.9964    \n",
      " 326 sec\n",
      "***** logistic (post-train) *****\n",
      "training until loss <= 0.2664\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2731 - acc: 0.9234     \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2678 - acc: 0.9249     \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2641 - acc: 0.9268     \n",
      "   4 sec\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopLambda(keras.callbacks.Callback):\n",
    "    def __init__(self, metric='loss', should_stop=None):\n",
    "        super(EarlyStopLambda, self).__init__()\n",
    "        self.metric = metric\n",
    "        self.should_stop = should_stop\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.metric)\n",
    "        if self.should_stop(current):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "post_train = {name: '../models/mnist/' + name + '-best-post-train.hdf5' for name in names}\n",
    "for name in names:\n",
    "    print('*****', name, '(post-train) *****')\n",
    "    \n",
    "    K.clear_session()\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    net = generate_network(name)\n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    net.load_weights(save_files[name])\n",
    "    train_metrics = dict(zip(net.metrics_names, net.evaluate(trainx, trainy, verbose=0)))\n",
    "    \n",
    "    print('training until loss <= {:0.4f}'.format(train_metrics['loss']))\n",
    "    early_stopping = EarlyStopLambda(metric='loss', should_stop=lambda loss: loss <= train_metrics['loss'])\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(post_train[name], monitor='loss', save_best_only=True)\n",
    "    with rectime():\n",
    "        net.fit(\n",
    "            train_and_validx,\n",
    "            train_and_validy,\n",
    "            epochs=100,\n",
    "            batch_size=128, # that k80 12GB mem tho\n",
    "            callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "| model    |   test acc |   # params |\n",
      "+==========+============+============+\n",
      "| logistic |     0.9239 |       7850 |\n",
      "+----------+------------+------------+\n",
      "| mlp-6    |     0.9737 |    1919410 |\n",
      "+----------+------------+------------+\n",
      "| google   |     0.9925 |    3274634 |\n",
      "+----------+------------+------------+\n",
      "| lc       |     0.9930 |   10312074 |\n",
      "+----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "final_acc = {}\n",
    "final_pred = {}\n",
    "num_params = {}\n",
    "for name in names:\n",
    "    K.clear_session()\n",
    "    net = generate_network(name)\n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    num_params[name] = sum(x.size for x in net.get_weights())\n",
    "    net.load_weights(post_train[name])\n",
    "    final_pred[name] = net.predict(testx)\n",
    "    test_metrics = dict(zip(net.metrics_names, net.evaluate(testx, testy, verbose=0)))\n",
    "    final_acc[name] = test_metrics['acc']\n",
    "results = [[name, acc, num_params[name]] for name, acc in final_acc.items()]\n",
    "results.sort(key=lambda row: row[1])\n",
    "print(tabulate(results, ['model', 'test acc', '# params'], tablefmt='grid', floatfmt=\"0.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation for lc\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAEfCAYAAAD/ZitLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ4krIjoniSsiOuf/A39Ue5YHKmh8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb4019e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|   weighted roc auc |   accuracy |\n",
      "+====================+============+\n",
      "|           0.999945 |      0.993 |\n",
      "+--------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "print('evaluation for lc')\n",
    "evaluate(testy, final_pred['lc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo do this via tensorboard?\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'block5_conv1'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(0, 200):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 8\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "imsave('stitched_filters_%dx%d.png' % (n, n), stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_2/Relu:0' shape=(?, 14, 14, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "tf.image_summary('conv1/filters', kernel_transposed, max_images=3)\n",
    "\n",
    "K.get_session().run(net.layers[4:7][0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate architectures (but still scoped)\n",
    "# and a special version of LeNet I call what-I-vaguely-remember-of-LeNet-net.\n",
    "\n",
    "# NO MORE TENSORLAYER - TF-slim.\n",
    "\n",
    "# TODO smarter init\n",
    "# TODO class balancing?\n",
    "#http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/utils.html#class_balancing_oversample\n",
    "def mlp_drop3(input_layer, is_train=True):\n",
    "    with tf.variable_scope('mlp-dropout-3', reuse=True):\n",
    "        net = tl.layers.DropoutLayer(input_layer, 0.8, True, is_train, name='drop_input')\n",
    "        net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='dense1')\n",
    "        net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop1')\n",
    "        net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='dense2')\n",
    "        net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop2')\n",
    "        net = tl.layers.DenseLayer(net, n_units=10, act=tf.identity, name='dense3')\n",
    "        return net\n",
    "\n",
    "def wrap_net(network_generator, is_train=True, batch_size=None):\n",
    "    x = tf.placeholder(tf.float32, shape=[batch_size, 784], name='x')\n",
    "    y = tf.placeholder(tf.int64, shape=[batch_size], name='y')\n",
    "    net = network_generator(tl.layers.InputLayer(x, name='input_layer3'), is_train=is_train)\n",
    "    cost = tl.cost.cross_entropy(net.outputs, y, name='cost')\n",
    "    output = tf.argmax(tf.nn.softmax(net.outputs, name='output'), 1)\n",
    "    acc = tf.reduce_mean(output == y)\n",
    "    return net, cost, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_drop_graph = tf.Graph()\n",
    "\n",
    "with mlp_drop_graph.as_default():\n",
    "    net, cost, acc = wrap_net(mlp_drop3, is_train=True, batch_size=128)\n",
    "\n",
    "    params = network.all_params\n",
    "    train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate=0.001, beta1=0.9, beta2=0.99, epsilon=1e-8, use_locking=False).minimize(cost)\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "net.print_params()\n",
    "net.print_layers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/utils.html#fit and below\n",
    "# save to ../logs. Models can go in ../checkpoints (date and epoch count to delineate, too.)\n",
    "for epoch in range(10):\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(\n",
    "        X_train, y_train, batch_size, shuffle=True):\n",
    "            feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "            feed_dict.update(network.all_drop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO momentum cooling\n",
    "global_step = tf.Variable(\n",
    "    0, trainable=False)  # don't train a global step value\n",
    "initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    initial_learning_rate,\n",
    "    global_step,\n",
    "    decay_period,\n",
    "    decay_fraction,\n",
    "    staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "             .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_FOLDS = 4\n",
    "UPDATE_COARSENESS = 5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "top_models = []\n",
    "\n",
    "# Cross validate on random restarts\n",
    "for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "    print('Starting fold {}'.format(fold))\n",
    "    sess.run(tf.initialize_all_variables())  # random restart\n",
    "    best_in_fold_validation = np.inf\n",
    "    best_in_fold_epoch = None\n",
    "    best_in_fold_model = None\n",
    "\n",
    "    for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "        for batch in train.new_epoch(BATCH_SIZE):\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1]}, session=sess)\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            valid_err = validate.multiclass_error(x, sm.y, y, session=sess)\n",
    "            print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "            if valid_err > best_in_fold_validation: break\n",
    "            best_in_fold_validation = valid_err\n",
    "            best_in_fold_epoch = epoch\n",
    "            best_in_fold_model = saver.save(\n",
    "                sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "    top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "# We select the model based on the entire training data set rather than the error from the\n",
    "# cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "# compare across the folds the same accuracy since some folds might be easier than others.\n",
    "# Doing this helps the test accuracy by about 0.5%.\n",
    "best_fold, best_epoch, best_model = None, None, None\n",
    "best_error = np.inf\n",
    "for fold, epoch, model in top_models:\n",
    "    saver.restore(sess, model)\n",
    "    error = train.multiclass_error(x, sm.y, y, session=sess)\n",
    "    if best_error > error:\n",
    "        best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "\n",
    "print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "      .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "\n",
    "saver.restore(sess, best_model)\n",
    "\n",
    "print('Test error', test.multiclass_error(x, sm.y, y, session=sess))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_prefix = 'mnist-cnn-'\n",
    "cnn_suffix = '.ckpt'\n",
    "available_epochs = [x for x in os.listdir('../data')\n",
    "                    if x.startswith(cnn_prefix) and x.endswith(cnn_suffix)]\n",
    "print(available_epochs)\n",
    "\n",
    "def extract_epoch_number(s):\n",
    "    return int(s[len(cnn_prefix):-len(cnn_suffix)])\n",
    "\n",
    "available_epochs = [extract_epoch_number(s) for s in available_epochs]\n",
    "max_epoch = max(available_epochs or [None])\n",
    "print('{} saved epoch files; using largest epoch ({}) as start'\n",
    "      .format(len(available_epochs), max_epoch))\n",
    "\n",
    "def epoch_to_filename(e):\n",
    "    return '../data/' + cnn_prefix + str(e) + cnn_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Don't drop out when testing by setting keep_prob to 1.0\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if max_epoch:\n",
    "    saver.restore(sess, epoch_to_filename(max_epoch))\n",
    "    print('Restoring CNN from epoch {}'.format(max_epoch))\n",
    "else:\n",
    "    print('Starting CNN training from scratch')\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    max_epoch = 0\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 35\n",
    "UPDATE_COARSENESS = 5\n",
    "for epoch in range(max_epoch + 1, NUM_EPOCHS + 1):\n",
    "    for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "        tot_batches = train.size // BATCH_SIZE\n",
    "        two_percent_done = i * 50 // tot_batches\n",
    "        print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "               + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5}, session=sess)\n",
    "\n",
    "    print(']')\n",
    "    name = saver.save(sess, epoch_to_filename(epoch))\n",
    "    print('  Saved to', name)\n",
    "    if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "        err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)\n",
    "        print('  Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "\n",
    "print('Test error {}'.format(test.multiclass_error(\n",
    "            x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
