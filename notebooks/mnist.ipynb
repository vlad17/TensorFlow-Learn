{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This notebook somewhat follows the tutorial from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "#\n",
    "# The current working directory is expected to be $PROJECT_ROOT/notebooks\n",
    "\n",
    "import sys, os\n",
    "if '../code' not in sys.path: sys.path.append('../code')\n",
    "import mnist_downloader\n",
    "import numpy as np\n",
    "import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x            | y           |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "\n",
      "x float32 y float32\n"
     ]
    }
   ],
   "source": [
    "train, test = mnist_downloader.read_data_sets('../data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "from data_set import DataSet\n",
    "from tabulate import tabulate\n",
    "\n",
    "tbl = tabulate([['train', train.x.shape, train.y.shape],\n",
    "                ['test', test.x.shape, test.y.shape]],\n",
    "               ['Data Set', 'x', 'y'], tablefmt='grid')\n",
    "print('\\n{}\\n\\nx {!s} y {!s}'.format(tbl, train.x.dtype, train.y.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n",
      "  Epoch   5 error 0.08633333444595337\n",
      "  Epoch  10 error 0.08560001850128174\n",
      "  Epoch  15 error 0.08633333444595337\n",
      "Starting fold 2\n",
      "  Epoch   5 error 0.09353333711624146\n",
      "  Epoch  10 error 0.08240002393722534\n",
      "  Epoch  15 error 0.08939999341964722\n",
      "Starting fold 3\n",
      "  Epoch   5 error 0.09146666526794434\n",
      "  Epoch  10 error 0.08893334865570068\n",
      "  Epoch  15 error 0.08619999885559082\n",
      "  Epoch  20 error 0.0878000259399414\n",
      "Starting fold 4\n",
      "  Epoch   5 error 0.07946664094924927\n",
      "  Epoch  10 error 0.08373332023620605\n",
      "Fold 3/4 epoch 15/100 with inside training dataset-error 0.07136666774749756\n",
      "Test error 0.0795999765396\n"
     ]
    }
   ],
   "source": [
    "# TODO: switch to tf.reset_default_graph() and use with tf.Session() statements\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "    y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "    sm = regression.SoftMax(x, y)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False) # don't train a global step value\n",
    "    initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_period, decay_fraction, staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "                 .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    NUM_EPOCHS = 100\n",
    "    NUM_FOLDS = 4\n",
    "    UPDATE_COARSENESS = 5\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    top_models = []\n",
    "\n",
    "    # Cross validate on random restarts\n",
    "    for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "        print('Starting fold {}'.format(fold))\n",
    "        sess.run(tf.initialize_all_variables()) # random restart\n",
    "        best_in_fold_validation = np.inf\n",
    "        best_in_fold_epoch = None\n",
    "        best_in_fold_model = None\n",
    "        for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "            for batch in train.new_epoch(BATCH_SIZE):\n",
    "                train_step.run(feed_dict={x: batch[0], y: batch[1]})\n",
    "            if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "                valid_err = validate.multiclass_error(x, sm.y, y)\n",
    "                print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "                if valid_err > best_in_fold_validation: break\n",
    "                best_in_fold_validation = valid_err\n",
    "                best_in_fold_epoch = epoch\n",
    "                best_in_fold_model = saver.save(sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "        top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "    # We select the model based on the entire training data set rather than the error from the\n",
    "    # cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "    # compare across the folds the same accuracy since some folds might be easier than others.\n",
    "    # Doing this helps the test accuracy by about 0.5%.\n",
    "    best_fold, best_epoch, best_model = None, None, None\n",
    "    best_error = np.inf\n",
    "    for fold, epoch, model in top_models:\n",
    "        saver.restore(sess, model)\n",
    "        error = train.multiclass_error(x, sm.y, y)\n",
    "        if best_error > error:\n",
    "            best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "\n",
    "    print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "          .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "\n",
    "    saver.restore(sess, best_model)\n",
    "\n",
    "    print('Test error', test.multiclass_error(x, sm.y, y))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mnist-cnn-35.ckpt']\n",
      "1 saved epoch files; using largest epoch (35) as start\n"
     ]
    }
   ],
   "source": [
    "cnn_prefix = 'mnist-cnn-'\n",
    "cnn_suffix = '.ckpt'\n",
    "available_epochs = [x for x in os.listdir('../data')\n",
    "                    if x.startswith(cnn_prefix) and x.endswith(cnn_suffix)]\n",
    "print(available_epochs)\n",
    "\n",
    "def extract_epoch_number(s):\n",
    "    return int(s[len(cnn_prefix):-len(cnn_suffix)])\n",
    "\n",
    "available_epochs = [extract_epoch_number(s) for s in available_epochs]\n",
    "max_epoch = max(available_epochs or [None])\n",
    "print('{} saved epoch files; using largest epoch ({}) as start'\n",
    "      .format(len(available_epochs), max_epoch))\n",
    "\n",
    "def epoch_to_filename(e):\n",
    "    return '../data/' + cnn_prefix + str(e) + cnn_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring CNN from epoch 35\n",
      "Test error 0.008800029754638672\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "# TODO: would be interesting to see how a max filter performs instead\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "    y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    \n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # Don't drop out when testing by setting keep_prob to 1.0\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    output = regression.SoftMax(h_fc1_drop, y)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if max_epoch:\n",
    "        saver.restore(sess, epoch_to_filename(max_epoch))\n",
    "        print('Restoring CNN from epoch {}'.format(max_epoch))\n",
    "    else:\n",
    "        print('Starting CNN training from scratch')\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        max_epoch = 0\n",
    "    \n",
    "    BATCH_SIZE = 100\n",
    "    NUM_EPOCHS = 35\n",
    "    UPDATE_COARSENESS = 5\n",
    "    for epoch in range(max_epoch + 1, NUM_EPOCHS + 1):\n",
    "        for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "            tot_batches = train.size // BATCH_SIZE\n",
    "            two_percent_done = i * 50 // tot_batches\n",
    "            print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "                   + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        \n",
    "        print(']')\n",
    "        name = saver.save(sess, epoch_to_filename(epoch))\n",
    "        print('  Saved to', name)\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})\n",
    "            print('  Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "    \n",
    "    print('Test error {}'.format(test.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})))\n",
    "    \n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
