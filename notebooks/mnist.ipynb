{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook somewhat follows the tutorials from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "# TODO: code cleanup, dedup (move to code/ common parts)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "if '../code' not in sys.path:\n",
    "    sys.path.append('../code')\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport mnist_downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x float32    | y float32   |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "trainx, trainy, testx, testy = mnist_downloader.read_data_sets(\n",
    "    '../data', one_hot=True, exact_inputs=False)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(\n",
    "    [['train', trainx.shape, trainy.shape],\n",
    "     ['test', testx.shape, testy.shape]],\n",
    "    ['Data Set', 'x {}'.format(trainx.dtype), 'y {}'.format(trainy.dtype)],\n",
    "    tablefmt='grid'))\n",
    "\n",
    "# TODO check mnist balance; autobalance (look up techniques? papers, how does TF-slim do it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8af13fc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify dataset balance\n",
    "\n",
    "plt.hist(trainy.argmax(1), 10, facecolor='green', alpha=0.75)\n",
    "plt.axhline(len(trainy) / 10, color='red', ls=':')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('counts')\n",
    "plt.title(r'histogram of training labels')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_and_validx = trainx\n",
    "train_and_validy = trainy\n",
    "trainx, validx, trainy, validy = train_test_split(\n",
    "    train_and_validx, train_and_validy, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def evaluate(truey_onehot, predy_onehot):\n",
    "    truey = truey_onehot.argmax(1)\n",
    "    predy = predy_onehot.argmax(1)\n",
    "    \n",
    "    nclasses = 10\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(truey, predy).astype(float)\n",
    "    np.fill_diagonal(confusion, np.nan)\n",
    "    plt.imshow(confusion)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(nclasses))\n",
    "    plt.yticks(np.arange(nclasses))\n",
    "    plt.xlabel(r'$\\hat{y}$')\n",
    "    plt.ylabel(r'$y$')\n",
    "    plt.title(r'confusion counts ($n={}K$)'.format(len(validy)//1000))\n",
    "    plt.show()\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(truey, predy)\n",
    "    rocauc = sklearn.metrics.roc_auc_score(truey_onehot, predy_onehot, average='weighted')\n",
    "\n",
    "    print(tabulate([[rocauc, acc]], ['weighted roc auc', 'accuracy'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "def generate_network(name):\n",
    "    if name == 'google':\n",
    "        return Sequential([\n",
    "    Reshape([28, 28, 1], input_shape=(trainx.shape[1],)),\n",
    "    Conv2D(32, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    Conv2D(64, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    Reshape([28 * 28 * 64 // 4 // 4]),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "    elif name == 'mlp-6':\n",
    "        return Sequential([\n",
    "    Dense(600, activation='relu', input_dim=trainx.shape[1]),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "    elif name == 'lc':\n",
    "        return Sequential([\n",
    "        Reshape([28, 28, 1], input_shape=(trainx.shape[1],)),\n",
    "        Conv2D(64, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "        LocallyConnected2D(64, [5, 5], strides=(1, 1), padding='valid', activation='relu'),\n",
    "        Reshape([(28 // 2 - 5 + 1) ** 2 * 64]),\n",
    "        Dropout(0.8),            \n",
    "    Dense(10, activation='softmax')\n",
    "        ])\n",
    "    elif name == 'logistic':\n",
    "        return Sequential(\n",
    "        [Dense(10, input_dim=trainx.shape[1]),\n",
    "     Activation('softmax')])\n",
    "    else:\n",
    "        raise ValueError('Invalid net name ' + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** google *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 8s - loss: 0.1883 - acc: 0.9422 - val_loss: 0.0591 - val_acc: 0.9809\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 6s - loss: 0.0575 - acc: 0.9824 - val_loss: 0.0380 - val_acc: 0.9878\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 6s - loss: 0.0387 - acc: 0.9879 - val_loss: 0.0308 - val_acc: 0.9906\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 5s - loss: 0.0277 - acc: 0.9912 - val_loss: 0.0316 - val_acc: 0.9897\n",
      "  27 sec\n",
      "***** mlp-6 *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s - loss: 0.2696 - acc: 0.9186 - val_loss: 0.1291 - val_acc: 0.9607\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s - loss: 0.1121 - acc: 0.9664 - val_loss: 0.1411 - val_acc: 0.9591\n",
      "   5 sec\n",
      "***** lc *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.2574 - acc: 0.9223 - val_loss: 0.0708 - val_acc: 0.9786\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.0874 - acc: 0.9727 - val_loss: 0.0509 - val_acc: 0.9847\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.0661 - acc: 0.9798 - val_loss: 0.0434 - val_acc: 0.9868\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.0537 - acc: 0.9839 - val_loss: 0.0386 - val_acc: 0.9877\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.0451 - acc: 0.9855 - val_loss: 0.0354 - val_acc: 0.9891\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.0404 - acc: 0.9873 - val_loss: 0.0334 - val_acc: 0.9895\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 17s - loss: 0.0353 - acc: 0.9890 - val_loss: 0.0310 - val_acc: 0.9904\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 16s - loss: 0.0313 - acc: 0.9897 - val_loss: 0.0329 - val_acc: 0.9903\n",
      " 137 sec\n",
      "***** logistic *****\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.7788 - acc: 0.8106 - val_loss: 0.4529 - val_acc: 0.8848\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.4006 - acc: 0.8952 - val_loss: 0.3647 - val_acc: 0.9010\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3445 - acc: 0.9068 - val_loss: 0.3316 - val_acc: 0.9083\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3185 - acc: 0.9136 - val_loss: 0.3143 - val_acc: 0.9118\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3027 - acc: 0.9178 - val_loss: 0.3059 - val_acc: 0.9126\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2920 - acc: 0.9192 - val_loss: 0.2972 - val_acc: 0.9153\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2840 - acc: 0.9221 - val_loss: 0.2922 - val_acc: 0.9172\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2777 - acc: 0.9234 - val_loss: 0.2896 - val_acc: 0.9186\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2727 - acc: 0.9245 - val_loss: 0.2855 - val_acc: 0.9183\n",
      "   9 sec\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ../models && mkdir -p ../models/mnist\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from keras import backend as K\n",
    "\n",
    "@contextmanager\n",
    "def rectime(name='', fmt='{: 4.0f}'):\n",
    "    print(name, end='')\n",
    "    sys.stdout.flush()\n",
    "    t = time.time()\n",
    "    yield\n",
    "    t = time.time() - t\n",
    "    print(fmt.format(t), 'sec')\n",
    "    \n",
    "names = ['google', 'mlp-6', 'lc', 'logistic'] # convert all to dictionary above so .keys() here\n",
    "save_files = {name: '../models/mnist/' + name + '-best.hdf5' for name in names}\n",
    "for name in names:\n",
    "    print('*****', name, '*****')\n",
    "\n",
    "    K.clear_session()\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    net = generate_network(name)\n",
    "    net.compile(\n",
    "        loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc', min_delta=0, patience=0)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(save_files[name], monitor='val_loss', save_best_only=True)\n",
    "    # TODO more templates to have: TensorBoard on aws; filters (logistic?)\n",
    "    # feeder queues\n",
    "    # norms over gradients (tensorboard example?)\n",
    "    # widgets investigate\n",
    "    # investigate model fits on gpu, but minibatch doesn't (need to split it up b/w devices?)\n",
    "    # -> https://github.com/tensorflow/models/tree/master/inception#how-to-fine-tune-a-pre-trained-model-on-a-new-task\n",
    "    # compress this routine and the one below. Move nets to dictionaries?\n",
    "    # history recorder\n",
    "    with rectime():\n",
    "        net.fit(\n",
    "            trainx,\n",
    "            trainy,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "            validation_data=(validx, validy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** google (post-train) *****\n",
      "training until loss <= 0.0161\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.0366 - acc: 0.9884     \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.0238 - acc: 0.9927     \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.0192 - acc: 0.9939     \n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.0137 - acc: 0.9953     \n",
      "  31 sec\n",
      "***** mlp-6 (post-train) *****\n",
      "training until loss <= 0.1013\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s - loss: 0.1273 - acc: 0.9639     \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s - loss: 0.0818 - acc: 0.9766     \n",
      "   6 sec\n",
      "***** lc (post-train) *****\n",
      "training until loss <= 0.0126\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0385 - acc: 0.9880    \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0347 - acc: 0.9887    \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0306 - acc: 0.9900    \n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0272 - acc: 0.9914    \n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0236 - acc: 0.9920    \n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0217 - acc: 0.9930    \n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0183 - acc: 0.9944    \n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0191 - acc: 0.9939    \n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0160 - acc: 0.9944    \n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0157 - acc: 0.9948    \n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0141 - acc: 0.9953    \n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0122 - acc: 0.9963    \n",
      " 267 sec\n",
      "***** logistic (post-train) *****\n",
      "training until loss <= 0.2664\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2731 - acc: 0.9234     \n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2678 - acc: 0.9249     \n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s - loss: 0.2641 - acc: 0.9268     \n",
      "   3 sec\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopLambda(keras.callbacks.Callback):\n",
    "    def __init__(self, metric='loss', should_stop=None):\n",
    "        super(EarlyStopLambda, self).__init__()\n",
    "        self.metric = metric\n",
    "        self.should_stop = should_stop\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.metric)\n",
    "        if self.should_stop(current):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "post_train = {name: '../models/mnist/' + name + '-best-post-train.hdf5' for name in names}\n",
    "for name in names:\n",
    "    print('*****', name, '(post-train) *****')\n",
    "    \n",
    "    K.clear_session()\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    net = generate_network(name)\n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    net.load_weights(save_files[name])\n",
    "    train_metrics = dict(zip(net.metrics_names, net.evaluate(trainx, trainy, verbose=0)))\n",
    "    \n",
    "    print('training until loss <= {:0.4f}'.format(train_metrics['loss']))\n",
    "    early_stopping = EarlyStopLambda(metric='loss', should_stop=lambda loss: loss <= train_metrics['loss'])\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(post_train[name], monitor='loss', save_best_only=True)\n",
    "    with rectime():\n",
    "        net.fit(\n",
    "            train_and_validx,\n",
    "            train_and_validy,\n",
    "            epochs=100,\n",
    "            batch_size=128, # that k80 12GB mem tho\n",
    "            callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "| model    |   test acc |   # params |\n",
      "+==========+============+============+\n",
      "| logistic |     0.9239 |       7850 |\n",
      "+----------+------------+------------+\n",
      "| mlp-6    |     0.9745 |    1919410 |\n",
      "+----------+------------+------------+\n",
      "| google   |     0.9927 |    3274634 |\n",
      "+----------+------------+------------+\n",
      "| lc       |     0.9927 |   10312074 |\n",
      "+----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "final_acc = {}\n",
    "final_pred = {}\n",
    "num_params = {}\n",
    "for name in names:\n",
    "    K.clear_session()\n",
    "    net = generate_network(name)\n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    num_params[name] = sum(x.size for x in net.get_weights())\n",
    "    net.load_weights(post_train[name])\n",
    "    final_pred[name] = net.predict(testx)\n",
    "    test_metrics = dict(zip(net.metrics_names, net.evaluate(testx, testy, verbose=0)))\n",
    "    final_acc[name] = test_metrics['acc']\n",
    "results = [[name, acc, num_params[name]] for name, acc in final_acc.items()]\n",
    "results.sort(key=lambda row: row[1])\n",
    "print(tabulate(results, ['model', 'test acc', '# params'], tablefmt='grid', floatfmt=\"0.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation for lc\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAEfCAYAAAD/ZitLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ4krIjoniSsiOuf/A39Ue5YHKmh8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb4019e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|   weighted roc auc |   accuracy |\n",
      "+====================+============+\n",
      "|           0.999945 |      0.993 |\n",
      "+--------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "print('evaluation for lc')\n",
    "evaluate(testy, final_pred['lc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo do this via tensorboard?\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'block5_conv1'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(0, 200):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 8\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "imsave('stitched_filters_%dx%d.png' % (n, n), stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_2/Relu:0' shape=(?, 14, 14, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "tf.image_summary('conv1/filters', kernel_transposed, max_images=3)\n",
    "\n",
    "K.get_session().run(net.layers[4:7][0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate architectures (but still scoped)\n",
    "# and a special version of LeNet I call what-I-vaguely-remember-of-LeNet-net.\n",
    "\n",
    "# NO MORE TENSORLAYER - TF-slim.\n",
    "\n",
    "# TODO smarter init\n",
    "# TODO class balancing?\n",
    "#http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/utils.html#class_balancing_oversample\n",
    "def mlp_drop3(input_layer, is_train=True):\n",
    "    with tf.variable_scope('mlp-dropout-3', reuse=True):\n",
    "        net = tl.layers.DropoutLayer(input_layer, 0.8, True, is_train, name='drop_input')\n",
    "        net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='dense1')\n",
    "        net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop1')\n",
    "        net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='dense2')\n",
    "        net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop2')\n",
    "        net = tl.layers.DenseLayer(net, n_units=10, act=tf.identity, name='dense3')\n",
    "        return net\n",
    "\n",
    "def wrap_net(network_generator, is_train=True, batch_size=None):\n",
    "    x = tf.placeholder(tf.float32, shape=[batch_size, 784], name='x')\n",
    "    y = tf.placeholder(tf.int64, shape=[batch_size], name='y')\n",
    "    net = network_generator(tl.layers.InputLayer(x, name='input_layer3'), is_train=is_train)\n",
    "    cost = tl.cost.cross_entropy(net.outputs, y, name='cost')\n",
    "    output = tf.argmax(tf.nn.softmax(net.outputs, name='output'), 1)\n",
    "    acc = tf.reduce_mean(output == y)\n",
    "    return net, cost, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_drop_graph = tf.Graph()\n",
    "\n",
    "with mlp_drop_graph.as_default():\n",
    "    net, cost, acc = wrap_net(mlp_drop3, is_train=True, batch_size=128)\n",
    "\n",
    "    params = network.all_params\n",
    "    train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate=0.001, beta1=0.9, beta2=0.99, epsilon=1e-8, use_locking=False).minimize(cost)\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "net.print_params()\n",
    "net.print_layers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/utils.html#fit and below\n",
    "# save to ../logs. Models can go in ../checkpoints (date and epoch count to delineate, too.)\n",
    "for epoch in range(10):\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(\n",
    "        X_train, y_train, batch_size, shuffle=True):\n",
    "            feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "            feed_dict.update(network.all_drop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO momentum cooling\n",
    "global_step = tf.Variable(\n",
    "    0, trainable=False)  # don't train a global step value\n",
    "initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    initial_learning_rate,\n",
    "    global_step,\n",
    "    decay_period,\n",
    "    decay_fraction,\n",
    "    staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "             .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_FOLDS = 4\n",
    "UPDATE_COARSENESS = 5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "top_models = []\n",
    "\n",
    "# Cross validate on random restarts\n",
    "for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "    print('Starting fold {}'.format(fold))\n",
    "    sess.run(tf.initialize_all_variables())  # random restart\n",
    "    best_in_fold_validation = np.inf\n",
    "    best_in_fold_epoch = None\n",
    "    best_in_fold_model = None\n",
    "\n",
    "    for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "        for batch in train.new_epoch(BATCH_SIZE):\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1]}, session=sess)\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            valid_err = validate.multiclass_error(x, sm.y, y, session=sess)\n",
    "            print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "            if valid_err > best_in_fold_validation: break\n",
    "            best_in_fold_validation = valid_err\n",
    "            best_in_fold_epoch = epoch\n",
    "            best_in_fold_model = saver.save(\n",
    "                sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "    top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "# We select the model based on the entire training data set rather than the error from the\n",
    "# cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "# compare across the folds the same accuracy since some folds might be easier than others.\n",
    "# Doing this helps the test accuracy by about 0.5%.\n",
    "best_fold, best_epoch, best_model = None, None, None\n",
    "best_error = np.inf\n",
    "for fold, epoch, model in top_models:\n",
    "    saver.restore(sess, model)\n",
    "    error = train.multiclass_error(x, sm.y, y, session=sess)\n",
    "    if best_error > error:\n",
    "        best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "\n",
    "print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "      .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "\n",
    "saver.restore(sess, best_model)\n",
    "\n",
    "print('Test error', test.multiclass_error(x, sm.y, y, session=sess))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_prefix = 'mnist-cnn-'\n",
    "cnn_suffix = '.ckpt'\n",
    "available_epochs = [x for x in os.listdir('../data')\n",
    "                    if x.startswith(cnn_prefix) and x.endswith(cnn_suffix)]\n",
    "print(available_epochs)\n",
    "\n",
    "def extract_epoch_number(s):\n",
    "    return int(s[len(cnn_prefix):-len(cnn_suffix)])\n",
    "\n",
    "available_epochs = [extract_epoch_number(s) for s in available_epochs]\n",
    "max_epoch = max(available_epochs or [None])\n",
    "print('{} saved epoch files; using largest epoch ({}) as start'\n",
    "      .format(len(available_epochs), max_epoch))\n",
    "\n",
    "def epoch_to_filename(e):\n",
    "    return '../data/' + cnn_prefix + str(e) + cnn_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Don't drop out when testing by setting keep_prob to 1.0\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if max_epoch:\n",
    "    saver.restore(sess, epoch_to_filename(max_epoch))\n",
    "    print('Restoring CNN from epoch {}'.format(max_epoch))\n",
    "else:\n",
    "    print('Starting CNN training from scratch')\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    max_epoch = 0\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 35\n",
    "UPDATE_COARSENESS = 5\n",
    "for epoch in range(max_epoch + 1, NUM_EPOCHS + 1):\n",
    "    for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "        tot_batches = train.size // BATCH_SIZE\n",
    "        two_percent_done = i * 50 // tot_batches\n",
    "        print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "               + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5}, session=sess)\n",
    "\n",
    "    print(']')\n",
    "    name = saver.save(sess, epoch_to_filename(epoch))\n",
    "    print('  Saved to', name)\n",
    "    if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "        err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)\n",
    "        print('  Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "\n",
    "print('Test error {}'.format(test.multiclass_error(\n",
    "            x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
