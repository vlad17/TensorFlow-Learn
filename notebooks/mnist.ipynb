{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This notebook somewhat follows the tutorial from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "#\n",
    "# The current working directory is expected to be $PROJECT_ROOT/notebooks\n",
    "\n",
    "import sys, os\n",
    "if '../code' not in sys.path: sys.path.append('../code')\n",
    "import mnist_downloader\n",
    "import numpy as np\n",
    "import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x            | y           |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "\n",
      "x float32 y float32\n"
     ]
    }
   ],
   "source": [
    "train, test = mnist_downloader.read_data_sets('../data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "from data_set import DataSet\n",
    "from tabulate import tabulate\n",
    "\n",
    "tbl = tabulate([['train', train.x.shape, train.y.shape],\n",
    "                ['test', test.x.shape, test.y.shape]],\n",
    "               ['Data Set', 'x', 'y'], tablefmt='grid')\n",
    "print('\\n{}\\n\\nx {!s} y {!s}'.format(tbl, train.x.dtype, train.y.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n",
      "  Epoch   5 error 0.08766669034957886\n",
      "  Epoch  10 error 0.0867999792098999\n",
      "  Epoch  15 error 0.08673334121704102\n",
      "  Epoch  20 error 0.08893334865570068\n",
      "Starting fold 2\n",
      "  Epoch   5 error 0.0843999981880188\n",
      "  Epoch  10 error 0.08013331890106201\n",
      "  Epoch  15 error 0.0873333215713501\n",
      "Starting fold 3\n",
      "  Epoch   5 error 0.09593331813812256\n",
      "  Epoch  10 error 0.091533362865448\n",
      "  Epoch  15 error 0.08920001983642578\n",
      "  Epoch  20 error 0.09240001440048218\n",
      "Starting fold 4\n",
      "  Epoch   5 error 0.08606666326522827\n",
      "  Epoch  10 error 0.08053332567214966\n",
      "  Epoch  15 error 0.08819997310638428\n",
      "Fold 3/4 epoch 15/100 with inside training dataset-error 0.07289999723434448\n",
      "Test error 0.081200003624\n"
     ]
    }
   ],
   "source": [
    "# TODO: switch to tf.reset_default_graph() and use with tf.Session() statements\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "    y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "    sm = regression.SoftMax(x, y)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False) # don't train a global step value\n",
    "    initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_period, decay_fraction, staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "                 .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    NUM_EPOCHS = 100\n",
    "    NUM_FOLDS = 4\n",
    "    UPDATE_COARSENESS = 5\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    top_models = []\n",
    "\n",
    "    # Cross validate on random restarts\n",
    "    for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "        print('Starting fold {}'.format(fold))\n",
    "        sess.run(tf.initialize_all_variables()) # random restart\n",
    "        best_in_fold_validation = np.inf\n",
    "        best_in_fold_epoch = None\n",
    "        best_in_fold_model = None\n",
    "        for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "            for batch in train.new_epoch(BATCH_SIZE):\n",
    "                train_step.run(feed_dict={x: batch[0], y: batch[1]})\n",
    "            if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "                valid_err = validate.multiclass_error(x, sm.y, y)\n",
    "                print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "                if valid_err > best_in_fold_validation: break\n",
    "                best_in_fold_validation = valid_err\n",
    "                best_in_fold_epoch = epoch\n",
    "                best_in_fold_model = saver.save(sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "        top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "    # We select the model based on the entire training data set rather than the error from the\n",
    "    # cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "    # compare across the folds the same accuracy since some folds might be easier than others.\n",
    "    # Doing this helps the test accuracy by about 0.5%.\n",
    "    best_fold, best_epoch, best_model = None, None, None\n",
    "    best_error = np.inf\n",
    "    for fold, epoch, model in top_models:\n",
    "        saver.restore(sess, model)\n",
    "        error = train.multiclass_error(x, sm.y, y)\n",
    "        if best_error > error:\n",
    "            best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "\n",
    "    print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "          .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "\n",
    "    saver.restore(sess, best_model)\n",
    "\n",
    "    print('Test error', test.multiclass_error(x, sm.y, y))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mnist-cnn-1.ckpt']\n",
      "1 saved epoch files; using largest epoch (1) as start\n"
     ]
    }
   ],
   "source": [
    "cnn_prefix = 'mnist-cnn-'\n",
    "cnn_suffix = '.ckpt'\n",
    "available_epochs = [x for x in os.listdir('../data')\n",
    "                    if x.startswith(cnn_prefix) and x.endswith(cnn_suffix)]\n",
    "print(available_epochs)\n",
    "def extract_epoch_number(s): return int(s[len(cnn_prefix):-len(cnn_suffix)])\n",
    "available_epochs = [extract_epoch_number(s) for s in available_epochs]\n",
    "max_epoch = max(available_epochs, default=None)\n",
    "print('{} saved epoch files; using largest epoch ({}) as start'\n",
    "      .format(len(available_epochs), max_epoch))\n",
    "def epoch_to_filename(e): return '../data/' + cnn_prefix + str(e) + cnn_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring CNN from epoch 1\n",
      "Epoch 2/35 [--------------------------------------------------] 600/600]\n",
      "  Saved to ../data/mnist-cnn-2.ckpt\n",
      "Epoch 3/35 [                                                  ] 8/600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-81584875ac01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m             print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n\u001b[0;32m     66\u001b[0m                    + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m']'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_to_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1459\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \"\"\"\n\u001b[1;32m-> 1461\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3367\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3368\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3369\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 628\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "# TODO: would be interesting to see how a max filter performs instead\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "    y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Don't drop out when testing by setting keep_prob to 1.0\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if max_epoch:\n",
    "        saver.restore(sess, epoch_to_filename(max_epoch))\n",
    "        print('Restoring CNN from epoch {}'.format(max_epoch))\n",
    "    else:\n",
    "        print('Starting CNN training from scratch')\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        max_epoch = 0\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "    NUM_EPOCHS = 35\n",
    "    UPDATE_COARSENESS = 5\n",
    "    for epoch in range(max_epoch + 1, NUM_EPOCHS + 1):\n",
    "        for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "            tot_batches = train.size // BATCH_SIZE\n",
    "            two_percent_done = i * 50 // tot_batches\n",
    "            print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "                   + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        print(']')\n",
    "        name = saver.save(sess, epoch_to_filename(epoch))\n",
    "        print('  Saved to', name)\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})\n",
    "            print('  Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "\n",
    "    print('Test error {}'.format(test.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})))\n",
    "\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
