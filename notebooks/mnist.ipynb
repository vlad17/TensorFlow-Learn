{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This notebook somewhat follows the tutorial from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "#\n",
    "# The current working directory is expected to be $PROJECT_ROOT/notebooks\n",
    "\n",
    "import sys\n",
    "if '../code' not in sys.path: sys.path.append('../code')\n",
    "import mnist_downloader\n",
    "import numpy as np\n",
    "import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x            | y           |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "\n",
      "x float32 y float32\n"
     ]
    }
   ],
   "source": [
    "train, test = mnist_downloader.read_data_sets('../data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "from data_set import DataSet\n",
    "from tabulate import tabulate\n",
    "\n",
    "tbl = tabulate([['train', train.x.shape, train.y.shape],\n",
    "                ['test', test.x.shape, test.y.shape]],\n",
    "               ['Data Set', 'x', 'y'], tablefmt='grid')\n",
    "print('\\n{}\\n\\nx {!s} y {!s}'.format(tbl, train.x.dtype, train.y.dtype))\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n",
      "  Epoch   5 error 0.08646667003631592\n",
      "  Epoch  10 error 0.08566665649414062\n",
      "  Epoch  15 error 0.08560001850128174\n",
      "  Epoch  20 error 0.08506667613983154\n",
      "  Epoch  25 error 0.08753335475921631\n",
      "Starting fold 2\n",
      "  Epoch   5 error 0.08139997720718384\n",
      "  Epoch  10 error 0.08106666803359985\n",
      "  Epoch  15 error 0.08393335342407227\n",
      "Starting fold 3\n",
      "  Epoch   5 error 0.09513330459594727\n",
      "  Epoch  10 error 0.08840000629425049\n",
      "  Epoch  15 error 0.09433335065841675\n",
      "Starting fold 4\n",
      "  Epoch   5 error 0.08373332023620605\n",
      "  Epoch  10 error 0.08426666259765625\n",
      "Fold 1/4 epoch 20/100 with inside training dataset-error 0.0717666745185852\n",
      "Test error 0.0785999894142\n"
     ]
    }
   ],
   "source": [
    "sm = regression.SoftMax(x, y)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False) # don't train a global step value\n",
    "initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           decay_period, decay_fraction, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "             .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_FOLDS = 4\n",
    "UPDATE_COARSENESS = 5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "top_models = []\n",
    "\n",
    "# Cross validate on random restarts\n",
    "for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "    print('Starting fold {}'.format(fold))\n",
    "    sess.run(tf.initialize_all_variables()) # random restart\n",
    "    best_in_fold_validation = np.inf\n",
    "    best_in_fold_epoch = None\n",
    "    best_in_fold_model = None\n",
    "    for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "        for batch in train.new_epoch(BATCH_SIZE):\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1]})\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            valid_err = validate.multiclass_error(x, sm.y, y)\n",
    "            print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "            if valid_err > best_in_fold_validation: break\n",
    "            best_in_fold_validation = valid_err\n",
    "            best_in_fold_epoch = epoch\n",
    "            best_in_fold_model = saver.save(sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "    top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "# We select the model based on the entire training data set rather than the error from the\n",
    "# cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "# compare across the folds the same accuracy since some folds might be easier than others.\n",
    "# Doing this helps the test accuracy by about 0.5%.\n",
    "best_fold, best_epoch, best_model = None, None, None\n",
    "best_error = np.inf\n",
    "for fold, epoch, model in top_models:\n",
    "    saver.restore(sess, model)\n",
    "    error = train.multiclass_error(x, sm.y, y)\n",
    "    if best_error > error:\n",
    "        best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "            \n",
    "print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "      .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "        \n",
    "saver.restore(sess, best_model)\n",
    "\n",
    "print('Test error', test.multiclass_error(x, sm.y, y))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 2/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 3/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 4/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 5/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 0.012000024318695068\n",
      "  Epoch 6/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 7/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 8/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 9/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 10/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 0.0059999823570251465\n",
      "  Epoch 11/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 12/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 13/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 14/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 15/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 0.0025500059127807617\n",
      "  Epoch 16/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 17/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 18/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 19/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 20/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 0.0011500120162963867\n",
      "  Epoch 21/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 22/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 23/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 24/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 25/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 0.0004833340644836426\n",
      "  Epoch 26/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 27/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 28/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 29/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 30/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 0.0005166530609130859\n",
      "  Epoch 31/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 32/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 33/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 34/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "  Epoch 35/35\n",
      "    [--------------------------------------------------] 600/600]\n",
      "    Training error 8.33272933959961e-05\n",
      "Test error 0.008099973201751709\n"
     ]
    }
   ],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "# TODO max filter instead?\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Don't drop out when testing by setting keep_prob to 1.0\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# TODO add a saver here, this takes forever...\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 35\n",
    "UPDATE_COARSENESS = 5\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "        tot_batches = train.size // BATCH_SIZE\n",
    "        two_percent_done = i * 50 // tot_batches\n",
    "        print('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "              + '] {}/{}'.format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    print(']')\n",
    "    if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "        err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})\n",
    "        print('Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "    \n",
    "print('Test error {}'.format(test.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
