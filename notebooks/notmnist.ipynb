{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "apJbCsBHl-2A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Based on Google's Udacity Deep Learning course, Assignments 1-4\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "assert os.path.basename(os.getcwd()) == 'notebooks', os.path.basename(os.getcwd())\n",
    "code_dir = os.path.join(os.getcwd(), os.pardir, 'code')\n",
    "if code_dir not in sys.path:\n",
    "    sys.path.append(code_dir)\n",
    "    os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + os.pathsep + code_dir\n",
    "model_dir = os.path.join(os.getcwd(), os.pardir, 'models')\n",
    "data_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
    "   \n",
    "import keras\n",
    "%aimport keras_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large already extracted\n",
      "notMNIST_small already extracted\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import get_file\n",
    "import string\n",
    "\n",
    "def fullpath_listdir(dirname):\n",
    "    return [os.path.join(dirname, fname) for fname in os.listdir(dirname)]    \n",
    "\n",
    "def get_notmnist_tgz(base, expected_count, expected_classes):\n",
    "    notmnist_origin = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "    subdir = 'notmnist'\n",
    "    target = os.path.join(data_dir, subdir, base)\n",
    "    if os.path.isdir(target):\n",
    "        classdirs = fullpath_listdir(target)\n",
    "        num_class_files = [len(os.listdir(classdir)) for classdir in classdirs]\n",
    "        all_examples_present = all(num_files >= expected_count for num_files in num_class_files)\n",
    "        all_classes_present = all(i in os.listdir(target) for i in expected_classes)\n",
    "        if all_classes_present and all_examples_present:\n",
    "            print(base, 'already extracted')\n",
    "            return target\n",
    "    ext = '.tar.gz'\n",
    "    get_file(base + ext, notmnist_origin + base + ext, cache_dir=data_dir, cache_subdir=subdir, extract=True)\n",
    "    return target\n",
    "\n",
    "classes = string.ascii_uppercase[:10]\n",
    "train_dir = get_notmnist_tgz('notMNIST_large', 50000, classes)\n",
    "test_dir = get_notmnist_tgz('notMNIST_small', 1800, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAA/CAYAAADwizNIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\notAVFBQUegmKQldQUFDoJfw/YUKeO4Y5YecAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc196013668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images are 28x28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "classdirs = fullpath_listdir(train_dir)\n",
    "classdirs.sort()\n",
    "np.random.seed(1234)\n",
    "selected_images = [np.random.choice(fullpath_listdir(classdir)) for classdir in classdirs]\n",
    "\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# https://stackoverflow.com/questions/36006136/\n",
    "fig = plt.figure()\n",
    "for i, f in enumerate(selected_images, start=1):\n",
    "    ax = fig.add_subplot(1, len(selected_images), i)\n",
    "    plt.imshow(imread(f), cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "imheight, imwidth = imread(selected_images[0]).shape\n",
    "print('images are {}x{}'.format(imwidth, imheight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Ingestion\n",
    "\n",
    "Load all the pictures as greyscale matrices, and deduplicate exact matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import multiprocessing\n",
    "import contextlib\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from joblib import Memory\n",
    "\n",
    "onehot = MultiLabelBinarizer().fit(classes)\n",
    "\n",
    "def img_from_file(fname):\n",
    "    im = load_img(fname, grayscale=True, target_size=(imheight, imwidth))\n",
    "    return img_to_array(im, data_format='channels_last')\n",
    "\n",
    "train_classdirs = fullpath_listdir(train_dir)\n",
    "\n",
    "def no_rows(*shape):\n",
    "    shape_tup = (0, ) + tuple(shape)\n",
    "    return np.empty(shape=shape_tup, dtype=np.float32)\n",
    "\n",
    "\n",
    "def labelled_img(fname):\n",
    "    try:\n",
    "        label = os.path.basename(os.path.dirname(fname))\n",
    "        x = img_from_file(fname).reshape(1, imheight, imwidth, 1)\n",
    "        y = onehot.transform(label).astype(np.float32)\n",
    "        return x, y\n",
    "    except IOError as e:\n",
    "        print(e)\n",
    "    return no_rows(28, 28, 1), no_rows(10)\n",
    "\n",
    "memory = Memory(cachedir=os.path.join(train_dir, os.path.pardir), verbose=2)\n",
    "\n",
    "@memory.cache\n",
    "def load_raw(directory):\n",
    "    all_fnames = [\n",
    "        f for clsdir in fullpath_listdir(directory)\n",
    "        if os.path.isdir(clsdir) for f in fullpath_listdir(clsdir)\n",
    "    ]\n",
    "    with contextlib.closing(\n",
    "            multiprocessing.Pool(multiprocessing.cpu_count())) as pool:\n",
    "        xy = pool.map(labelled_img, all_fnames, chunksize=100)\n",
    "        return tuple(np.vstack(ls) for ls in zip(*xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]    0.0s, 0.0min: Loading load_raw...\n",
      "[Memory]    0.8s, 0.0min: Loading load_raw...\n",
      "+------------+---------------------+--------------+-------------+\n",
      "| data set   | x float32           | y float32    |   load time |\n",
      "+============+=====================+==============+=============+\n",
      "| train      | (529114, 28, 28, 1) | (529114, 10) |   0.814073  |\n",
      "+------------+---------------------+--------------+-------------+\n",
      "| test       | (18724, 28, 28, 1)  | (18724, 10)  |   0.0313581 |\n",
      "+------------+---------------------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import contexttimer\n",
    "from tabulate import tabulate\n",
    "\n",
    "with contexttimer.Timer() as train_time:\n",
    "    trainx, trainy = load_raw(train_dir)\n",
    "    \n",
    "with contexttimer.Timer() as test_time:\n",
    "    testx, testy = load_raw(test_dir)\n",
    "\n",
    "print(tabulate(\n",
    "    [['train', trainx.shape, trainy.shape, train_time.elapsed],\n",
    "     ['test', testx.shape, testy.shape, test_time.elapsed]],\n",
    "    ['data set', 'x {}'.format(trainx.dtype), 'y {}'.format(trainy.dtype), 'load time'],\n",
    "    tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]    0.9s, 0.0min: Loading unique_rows...\n",
      "dedup reduced training size from 529114 to 461946\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate exactly-equal images\n",
    "@memory.cache\n",
    "def unique_rows(matrix_name):\n",
    "    nparr = eval(matrix_name)\n",
    "    assert nparr.ndim > 1\n",
    "    matrix = nparr.reshape(len(nparr), -1)\n",
    "    _, ind = np.unique(matrix, axis=0, return_index=True)\n",
    "    return ind\n",
    "\n",
    "unique_ind = unique_rows('trainx')\n",
    "print('dedup reduced training size from {} to {}'.format(len(trainx), len(unique_ind)))\n",
    "trainx, trainy = trainx[unique_ind], trainy[unique_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_and_validx, train_and_validy = trainx, trainy\n",
    "trainx, validx, trainy, validy = train_test_split(trainx, trainy, test_size=0.3, random_state=5678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1 loss 14.5933 acc 0.0880\n",
      "step  500 loss  3.1910 acc 0.8014\n",
      "step 1000 loss  2.9232 acc 0.8180\n",
      "step 1500 loss  2.8299 acc 0.8238\n",
      "step 2000 loss  2.7948 acc 0.8260\n",
      "step 2500 loss  2.7853 acc 0.8267\n",
      "step 3000 loss  2.7817 acc 0.8270\n",
      "step 3500 loss  2.7796 acc 0.8272\n",
      "step 4000 loss  2.7773 acc 0.8274\n",
      "step 4500 loss  2.7754 acc 0.8276\n",
      "step 5000 loss  2.7731 acc 0.8277\n",
      "step 5500 loss  2.7851 acc 0.8267\n",
      "step 6000 loss  2.7700 acc 0.8279\n",
      "step 6500 loss  2.7683 acc 0.8280\n",
      "step 7000 loss  2.7666 acc 0.8282\n",
      "step 7500 loss  2.7655 acc 0.8283\n",
      "step 8000 loss  2.7650 acc 0.8284\n",
      "step 8500 loss  2.7648 acc 0.8284\n",
      "step 9000 loss  2.7647 acc 0.8285\n",
      "step 9500 loss  5.1118 acc 0.6816\n",
      "step 10000 loss  2.7645 acc 0.8284\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: Max. number of function evaluations reached\n",
      "  Objective function value: 2.764348\n",
      "  Number of iterations: 300\n",
      "  Number of functions evaluations: 10000\n",
      "logistic training took  677 sec\n",
      "18724/18724 [==============================] - 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.89756464958190918, 'loss': 1.6411033868789673}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop down to tf to use scipy optimization wrapper\n",
    "# Very slow to train unless using tnc\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_logistic(use_cached=True):\n",
    "    K.clear_session()\n",
    "    keras_util.seedall()\n",
    "\n",
    "    inputs = K.constant(train_and_validx)\n",
    "    logistic = Sequential(\n",
    "        [Flatten(input_shape=train_and_validx.shape[1:]),\n",
    "         Dense(10, activation='softmax', name='dense')])\n",
    "    if os.path.isfile('weights') and use_cached:\n",
    "        print('loading cached weights')\n",
    "        logistic.load_weights('weights')\n",
    "    else:\n",
    "        outputs = logistic(inputs)\n",
    "        labels = K.constant(train_and_validy)\n",
    "\n",
    "        loss = K.mean(categorical_crossentropy(labels, outputs))\n",
    "        acc = K.mean(\n",
    "                K.equal(K.argmax(labels, axis=1),\n",
    "                        K.argmax(outputs, axis=1)))\n",
    "        opt = tf.contrib.opt.ScipyOptimizerInterface(\n",
    "            loss, method='tnc', options={'maxiter': 10000, 'disp': True})\n",
    "        \n",
    "        tb = keras.callbacks.TensorBoard(log_dir='logs')\n",
    "        tb.set_model(logistic)\n",
    "\n",
    "        def make_loss_callback(step):\n",
    "            epoch = 0\n",
    "            def loss_callback(loss, acc):\n",
    "                nonlocal epoch, tb\n",
    "                logs = {'loss': loss, 'acc': acc}\n",
    "                tb.on_epoch_end(epoch, logs)\n",
    "                epoch += 1\n",
    "                if epoch % step == 0 or epoch == 1:\n",
    "                    print('step {:4d} loss {:7.4f} acc {:6.4f}'.format(epoch, loss, acc))\n",
    "            return loss_callback\n",
    "\n",
    "        K.get_session().run(tf.global_variables_initializer())\n",
    "        opt.minimize(\n",
    "            K.get_session(), fetches=[loss, acc], loss_callback=make_loss_callback(500))\n",
    "        tb.on_train_end(None)\n",
    "\n",
    "        logistic.save('weights')\n",
    "        \n",
    "    logistic.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return logistic\n",
    "\n",
    "with keras_util.rectime('logistic training took {:4.0f} sec'):\n",
    "    with keras_util.cd(os.path.join(model_dir, 'notmnist', 'logistic')):\n",
    "        logistic = get_logistic()\n",
    "keras_util.evaluate(logistic, testx, testy, batch_size=len(testx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet-like architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import conv_block, identity_block\n",
    "\n",
    "def greyscale_resnet(input_shape, nclasses):\n",
    "    \n",
    "    assert len(input_shape) == 3, input_shape\n",
    "    assert input_shape[-1] == 1, input_shape\n",
    "    \n",
    "    img = Input(shape=input_shape, dtype=K.floatx(), name='greyscale_input_img')\n",
    "\n",
    "    x = Conv2D(64, (7, 7), padding='same', name='conv1')(img)\n",
    "    x = BatchNormalization(axis=3, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    \n",
    "    x = conv_block(x, 3, [64, 64, 128], stage=2, block='conv')\n",
    "    for i in range(5):\n",
    "        x = identity_block(x, 3, [64, 64, 128], stage=2, block='id{}'.format(i))\n",
    "        \n",
    "    x = conv_block(x, 3, [128, 128, 256], stage=3, block='conv')\n",
    "    for i in range(5):\n",
    "        x = identity_block(x, 3, [128, 128, 256], stage=3, block='id{}'.format(i))\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(nclasses, activation='softmax', name='fc')(x)\n",
    "    \n",
    "    return Model(img, x, name='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Total params: 1,687,306\n",
      "Trainable params: 1,677,194\n",
      "Non-trainable params: 10,112\n",
      "____________________________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def fresh_resnet():\n",
    "    K.clear_session()\n",
    "    keras_util.seedall()\n",
    "\n",
    "    with keras_util.rectime('generating  {:4.0f} sec'):\n",
    "        greyres = greyscale_resnet(trainx.shape[1:], len(classes))\n",
    "\n",
    "    return greyres\n",
    "\n",
    "f = io.StringIO()\n",
    "with contextlib.redirect_stdout(f):\n",
    "    fresh_resnet().summary()\n",
    "print('\\n'.join(f.getvalue().split('\\n')[-6:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NCCL support available\n"
     ]
    }
   ],
   "source": [
    "import multigpu\n",
    "\n",
    "def serial_resnet():\n",
    "    greyres = fresh_resnet()\n",
    "\n",
    "    with keras_util.rectime('compiling   {:4.0f} sec'):\n",
    "        greyres.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "    return greyres\n",
    "\n",
    "def parallel_resnet():\n",
    "    greyres = fresh_resnet()\n",
    "\n",
    "    with keras_util.rectime('parallelize {:4.0f} sec'):\n",
    "        greyres = multigpu.make_parallel(greyres)\n",
    "\n",
    "    with keras_util.rectime('compiling   {:4.0f} sec'):\n",
    "        greyres.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "    return greyres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating     2 sec\n",
      "parallelize    3 sec\n",
      "compiling      1 sec\n",
      "warmup        29 sec\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 8s - loss: 1.3364 - acc: 0.7170     \n",
      "multi-gpu net expected epoch runtime 86 sec\n",
      "generating     2 sec\n",
      "compiling      0 sec\n",
      "warmup        28 sec\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 16s - loss: 1.2596 - acc: 0.7083    \n",
      "serial net expected epoch runtime 158 sec\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def minibench(model, name, batch_size):\n",
    "    steps = 16\n",
    "    gen = ImageDataGenerator().flow(trainx, trainy, batch_size=batch_size, seed=1234)\n",
    "    with keras_util.rectime('warmup      {:4.0f} sec'):\n",
    "        model.fit_generator(gen, steps_per_epoch=steps, epochs=1, verbose=0)\n",
    "    with contexttimer.Timer() as t:\n",
    "        model.fit_generator(gen, steps_per_epoch=steps, epochs=1)\n",
    "    expected = int(t.elapsed * len(trainx) / batch_size / steps)\n",
    "    print('{} expected epoch runtime {} sec'.format(name, expected))\n",
    "\n",
    "# Only start to see improvement in parallel net around large batches\n",
    "batch_size = 2048\n",
    "\n",
    "par_res = parallel_resnet()\n",
    "minibench(par_res, 'multi-gpu net', batch_size)\n",
    "\n",
    "ser_res = serial_resnet()\n",
    "minibench(ser_res, 'serial net', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating     2 sec\n",
      "parallelize    2 sec\n",
      "compiling      1 sec\n",
      "Epoch 1/500\n",
      "315/315 [==============================] - 115s - loss: 1.0668 - acc: 0.7533 - val_loss: 0.4443 - val_acc: 0.8712\n",
      "Epoch 2/500\n",
      "315/315 [==============================] - 107s - loss: 0.4721 - acc: 0.8565 - val_loss: 0.3613 - val_acc: 0.8916\n",
      "Epoch 3/500\n",
      "315/315 [==============================] - 107s - loss: 0.4078 - acc: 0.8749 - val_loss: 0.3255 - val_acc: 0.9010\n",
      "Epoch 4/500\n",
      "315/315 [==============================] - 107s - loss: 0.3673 - acc: 0.8868 - val_loss: 0.3075 - val_acc: 0.9053\n",
      "Epoch 5/500\n",
      "315/315 [==============================] - 107s - loss: 0.3411 - acc: 0.8936 - val_loss: 0.2965 - val_acc: 0.9076\n",
      "Epoch 6/500\n",
      "315/315 [==============================] - 107s - loss: 0.3239 - acc: 0.8987 - val_loss: 0.2847 - val_acc: 0.9124\n",
      "Epoch 7/500\n",
      "315/315 [==============================] - 107s - loss: 0.3098 - acc: 0.9033 - val_loss: 0.2660 - val_acc: 0.9180\n",
      "Epoch 8/500\n",
      "315/315 [==============================] - 107s - loss: 0.2972 - acc: 0.9070 - val_loss: 0.2619 - val_acc: 0.9192\n",
      "Epoch 9/500\n",
      "315/315 [==============================] - 108s - loss: 0.2867 - acc: 0.9098 - val_loss: 0.2603 - val_acc: 0.9200\n",
      "Epoch 10/500\n",
      "315/315 [==============================] - 107s - loss: 0.2809 - acc: 0.9118 - val_loss: 0.2511 - val_acc: 0.9222\n",
      "Epoch 11/500\n",
      "315/315 [==============================] - 107s - loss: 0.2716 - acc: 0.9141 - val_loss: 0.2456 - val_acc: 0.9229\n",
      "Epoch 12/500\n",
      "315/315 [==============================] - 107s - loss: 0.2665 - acc: 0.9158 - val_loss: 0.2673 - val_acc: 0.9189\n",
      "Epoch 13/500\n",
      "315/315 [==============================] - 108s - loss: 0.2608 - acc: 0.9171 - val_loss: 0.2404 - val_acc: 0.9248\n",
      "Epoch 14/500\n",
      "315/315 [==============================] - 107s - loss: 0.2542 - acc: 0.9193 - val_loss: 0.2322 - val_acc: 0.9269\n",
      "Epoch 15/500\n",
      "315/315 [==============================] - 107s - loss: 0.2506 - acc: 0.9204 - val_loss: 0.2317 - val_acc: 0.9275\n",
      "Epoch 16/500\n",
      "315/315 [==============================] - 107s - loss: 0.2466 - acc: 0.9214 - val_loss: 0.2357 - val_acc: 0.9257\n",
      "Epoch 17/500\n",
      "315/315 [==============================] - 108s - loss: 0.2421 - acc: 0.9225 - val_loss: 0.2320 - val_acc: 0.9278\n",
      "Epoch 18/500\n",
      "315/315 [==============================] - 107s - loss: 0.2382 - acc: 0.9240 - val_loss: 0.2285 - val_acc: 0.9282\n",
      "Epoch 19/500\n",
      "315/315 [==============================] - 107s - loss: 0.2341 - acc: 0.9248 - val_loss: 0.2345 - val_acc: 0.9263\n",
      "Epoch 20/500\n",
      "315/315 [==============================] - 107s - loss: 0.2313 - acc: 0.9263 - val_loss: 0.2242 - val_acc: 0.9294\n",
      "Epoch 21/500\n",
      "315/315 [==============================] - 108s - loss: 0.2298 - acc: 0.9267 - val_loss: 0.2259 - val_acc: 0.9295\n",
      "Epoch 22/500\n",
      "315/315 [==============================] - 108s - loss: 0.2230 - acc: 0.9288 - val_loss: 0.2206 - val_acc: 0.9307\n",
      "Epoch 23/500\n",
      "315/315 [==============================] - 108s - loss: 0.2213 - acc: 0.9289 - val_loss: 0.2205 - val_acc: 0.9312\n",
      "Epoch 24/500\n",
      "315/315 [==============================] - 107s - loss: 0.2190 - acc: 0.9297 - val_loss: 0.2170 - val_acc: 0.9311\n",
      "Epoch 25/500\n",
      "315/315 [==============================] - 108s - loss: 0.2138 - acc: 0.9317 - val_loss: 0.2165 - val_acc: 0.9323\n",
      "Epoch 26/500\n",
      "315/315 [==============================] - 107s - loss: 0.2111 - acc: 0.9321 - val_loss: 0.2235 - val_acc: 0.9287\n",
      "Epoch 27/500\n",
      "315/315 [==============================] - 107s - loss: 0.2097 - acc: 0.9331 - val_loss: 0.2205 - val_acc: 0.9330\n",
      "Epoch 28/500\n",
      "315/315 [==============================] - 107s - loss: 0.2056 - acc: 0.9336 - val_loss: 0.2194 - val_acc: 0.9303\n",
      "Epoch 29/500\n",
      "315/315 [==============================] - 108s - loss: 0.2026 - acc: 0.9350 - val_loss: 0.2155 - val_acc: 0.9332\n",
      "Epoch 30/500\n",
      "315/315 [==============================] - 108s - loss: 0.1986 - acc: 0.9361 - val_loss: 0.2058 - val_acc: 0.9358\n",
      "Epoch 31/500\n",
      "315/315 [==============================] - 107s - loss: 0.1964 - acc: 0.9368 - val_loss: 0.2134 - val_acc: 0.9340\n",
      "Epoch 32/500\n",
      "315/315 [==============================] - 108s - loss: 0.1959 - acc: 0.9369 - val_loss: 0.2204 - val_acc: 0.9327\n",
      "Epoch 33/500\n",
      "315/315 [==============================] - 107s - loss: 0.1919 - acc: 0.9377 - val_loss: 0.2093 - val_acc: 0.9348\n",
      "Epoch 34/500\n",
      "315/315 [==============================] - 108s - loss: 0.1899 - acc: 0.9389 - val_loss: 0.2068 - val_acc: 0.9357\n",
      "Epoch 35/500\n",
      "315/315 [==============================] - 108s - loss: 0.1864 - acc: 0.9399 - val_loss: 0.2067 - val_acc: 0.9357\n",
      "Epoch 36/500\n",
      "315/315 [==============================] - 109s - loss: 0.1837 - acc: 0.9409 - val_loss: 0.2088 - val_acc: 0.9362\n",
      "Epoch 37/500\n",
      "315/315 [==============================] - 108s - loss: 0.1597 - acc: 0.9478 - val_loss: 0.1979 - val_acc: 0.9408\n",
      "Epoch 38/500\n",
      "315/315 [==============================] - 108s - loss: 0.1492 - acc: 0.9510 - val_loss: 0.1990 - val_acc: 0.9410\n",
      "Epoch 39/500\n",
      "315/315 [==============================] - 108s - loss: 0.1455 - acc: 0.9523 - val_loss: 0.1994 - val_acc: 0.9414\n",
      "Epoch 40/500\n",
      "315/315 [==============================] - 108s - loss: 0.1417 - acc: 0.9535 - val_loss: 0.2028 - val_acc: 0.9414\n",
      "Epoch 41/500\n",
      "315/315 [==============================] - 108s - loss: 0.1395 - acc: 0.9542 - val_loss: 0.2024 - val_acc: 0.9414\n",
      "Epoch 42/500\n",
      "315/315 [==============================] - 108s - loss: 0.1364 - acc: 0.9550 - val_loss: 0.2027 - val_acc: 0.9420\n",
      "Epoch 43/500\n",
      "315/315 [==============================] - 107s - loss: 0.1351 - acc: 0.9556 - val_loss: 0.2029 - val_acc: 0.9420\n",
      "Epoch 44/500\n",
      "315/315 [==============================] - 108s - loss: 0.1308 - acc: 0.9571 - val_loss: 0.2052 - val_acc: 0.9422\n",
      "Epoch 45/500\n",
      "315/315 [==============================] - 108s - loss: 0.1300 - acc: 0.9573 - val_loss: 0.2054 - val_acc: 0.9421\n",
      "Epoch 46/500\n",
      "315/315 [==============================] - 108s - loss: 0.1293 - acc: 0.9573 - val_loss: 0.2053 - val_acc: 0.9421\n",
      "Epoch 47/500\n",
      "315/315 [==============================] - 108s - loss: 0.1292 - acc: 0.9576 - val_loss: 0.2058 - val_acc: 0.9422\n",
      "Epoch 48/500\n",
      "315/315 [==============================] - 108s - loss: 0.1295 - acc: 0.9574 - val_loss: 0.2057 - val_acc: 0.9421\n",
      "pre-training took 5215 sec\n"
     ]
    }
   ],
   "source": [
    "std_gen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    ")\n",
    "perturb_gen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1, \n",
    "    fill_mode='constant'\n",
    ")\n",
    "\n",
    "def fit_generators(x, *gens):\n",
    "    with warnings.catch_warnings():\n",
    "        # As of Aug 2017, keras fires a user warning when augmenting greyscale\n",
    "        # images, even though its API supports it. The spurious warning has\n",
    "        # been fixed but the fix isn't released yet.\n",
    "        warnings.filterwarnings(\"ignore\", message='Expected input to be images')\n",
    "        for gen in gens:\n",
    "            gen.fit(x)\n",
    "fit_generators(trainx, std_gen, perturb_gen)\n",
    "\n",
    "net = parallel_resnet()\n",
    "\n",
    "train_batch_size = 1024\n",
    "valid_batch_size = 5096\n",
    "train_gen = perturb_gen.flow(trainx, trainy, batch_size=train_batch_size, seed=5678)\n",
    "valid_gen = std_gen.flow(validx, validy, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "with keras_util.rectime('training took {:4.0f} sec'):\n",
    "    with keras_util.cd(os.path.join(model_dir, 'notmnist', 'residual')):\n",
    "        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "        lr = keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            'weights', monitor='val_loss', save_best_only=True)\n",
    "        tb = keras.callbacks.TensorBoard(log_dir='logs')\n",
    "        callbacks = [early_stopping, lr, checkpoint, tb]\n",
    "        net.fit_generator(\n",
    "            train_gen,\n",
    "            verbose=1,\n",
    "            steps_per_epoch=(len(trainx) // train_batch_size),\n",
    "            epochs=500,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=valid_gen,\n",
    "            validation_steps=(len(validx) // valid_batch_size))\n",
    "        net.load_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18724/18724 [==============================] - 2s     \n",
      "{'acc': 0.98328366001506462, 'loss': 0.059234185202003062}\n"
     ]
    }
   ],
   "source": [
    "def to_std(x, y):\n",
    "    gen = std_gen.flow(x, y, batch_size=len(x), shuffle=False)\n",
    "    return next(gen)\n",
    "\n",
    "# post-training (training on train + validation until loss on the\n",
    "# aggregate set is equal to post-training loss on train) doesn't help much:\n",
    "# only improves test acc by ~0.0001\n",
    "print(keras_util.evaluate(net, *to_std(testx, testy), \n",
    "                          batch_size=valid_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS image similarity\n",
    "\n",
    "The below isn't really relevant to the classification problem, just messing around with FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = trainx.reshape(trainx.shape[0], -1)\n",
    "num_vecs, vec_dim = train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "ngpu = keras_util.ngpu()\n",
    "# faiss uses swig, which fails unless we hold all these objects\n",
    "# in lists explicitly\n",
    "resources = [faiss.StandardGpuResources() for i in range(ngpu)]\n",
    "\n",
    "index = faiss.index_factory(vec_dim, 'IVF{},Flat'.format(int(np.sqrt(num_vecs) * 10)))\n",
    "index.nprobe = 10\n",
    "opts = faiss.GpuMultipleClonerOptions()\n",
    "index = faiss.index_cpu_to_gpu_multiple_py(resources, index, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contexttimer.Timer() as t:\n",
    "    index.train(train_matrix)\n",
    "    index.add(train_matrix)\n",
    "print('Trained image similarity index in {:.2f} sec'.format(t.elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "with contexttimer.Timer() as t:\n",
    "    distances, indices = index.search(train_matrix, k)\n",
    "print('Found top-{} similarity matches for all images in {:.2f} sec'.format(k, t.elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del index, resources, opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "np.random.seed(1234)\n",
    "nsamples = 5\n",
    "selected_images = np.random.choice(np.arange(len(indices)), nsamples)\n",
    "image_indices = np.concatenate([indices[i] for i in selected_images])\n",
    "for i, image_index in enumerate(image_indices, start=1):\n",
    "    ax = fig.add_subplot(nsamples, k, i)\n",
    "    plt.imshow(trainx[image_index], cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that L2 metric on the raw image data might not be enough since clearly different fonts for the same letter are viewed as neighbors, even though we'd like to match only with the same font that's been repeated between two images (but isn't an exact duplicate, since those were already filtered out)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
