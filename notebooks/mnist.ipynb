{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook somewhat follows the tutorials from here:\n",
    "# https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "if '../code' not in sys.path:\n",
    "    sys.path.append('../code')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport mnist_downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "+------------+--------------+-------------+\n",
      "| Data Set   | x float32    | y float32   |\n",
      "+============+==============+=============+\n",
      "| train      | (60000, 784) | (60000, 10) |\n",
      "+------------+--------------+-------------+\n",
      "| test       | (10000, 784) | (10000, 10) |\n",
      "+------------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "trainx, trainy, testx, testy = mnist_downloader.read_data_sets(\n",
    "    '../data', one_hot=True, exact_inputs=False)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "tbl = tabulate(\n",
    "    [['train', trainx.shape, trainy.shape],\n",
    "     ['test', testx.shape, testy.shape]],\n",
    "    ['Data Set', f'x {trainx.dtype!s}', f'y {trainy.dtype!s}'],\n",
    "    tablefmt='grid')\n",
    "print(tbl)\n",
    "\n",
    "# TODO check mnist balance; autobalance (look up techniques? papers, how does TF-slim do it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMDOzXBwcZmaWy/8HprIwY1N8/3wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc71af9e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify dataset balance\n",
    "\n",
    "plt.hist(trainy.argmax(1), 10, facecolor='green', alpha=0.75)\n",
    "plt.axhline(len(trainy) / 10, color='red', ls=':')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('counts')\n",
    "plt.title(r'histogram of training labels')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx, validx, trainy, validy = train_test_split(\n",
    "    trainx, trainy, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.5872 - acc: 0.8580     \n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.4021 - acc: 0.8955     \n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3673 - acc: 0.9022     \n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3490 - acc: 0.9067     \n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3369 - acc: 0.9094     \n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3286 - acc: 0.9105     \n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3219 - acc: 0.9122     \n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3167 - acc: 0.9139     \n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3123 - acc: 0.9149     \n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3085 - acc: 0.9157     \n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3054 - acc: 0.9166     \n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3026 - acc: 0.9173     \n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.3001 - acc: 0.9176     \n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2979 - acc: 0.9184     \n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2959 - acc: 0.9188     \n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2940 - acc: 0.9191     \n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2924 - acc: 0.9199     \n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2908 - acc: 0.9200     \n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2895 - acc: 0.9206     \n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2881 - acc: 0.9208     \n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 0s - loss: 0.2868 - acc: 0.9208     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc707452eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "logistic = Sequential(\n",
    "    [Dense(10, input_dim=trainx.shape[1]),\n",
    "     Activation('softmax')])\n",
    "logistic.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "\n",
    "# Since logistic is convex early stopping can be from training convergence\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='acc', min_delta=1e-4, patience=0)\n",
    "logistic.fit(trainx, trainy, epochs=100, batch_size=128, callbacks=[early_stopping]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation for logistic\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAEfCAYAAADP6jpqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc71b24fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|   weighted roc auc |   accuracy |\n",
      "+====================+============+\n",
      "|           0.993163 |   0.915722 |\n",
      "+--------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def evaluate(truey_onehot, predy_onehot):\n",
    "    truey = truey_onehot.argmax(1)\n",
    "    predy = predy_onehot.argmax(1)\n",
    "    \n",
    "    nclasses = 10\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(truey, predy).astype(float)\n",
    "    np.fill_diagonal(confusion, np.nan)\n",
    "    plt.imshow(confusion)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(nclasses))\n",
    "    plt.yticks(np.arange(nclasses))\n",
    "    plt.xlabel(r'$\\hat{y}$')\n",
    "    plt.ylabel(r'$y$')\n",
    "    plt.title(r'confusion counts ($n={}K$)'.format(len(validy)//1000))\n",
    "    plt.show()\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(truey, predy)\n",
    "    rocauc = sklearn.metrics.roc_auc_score(truey_onehot, predy_onehot, average='weighted')\n",
    "\n",
    "    print(tabulate([[rocauc, acc]], ['weighted roc auc', 'accuracy'], tablefmt='grid'))\n",
    "\n",
    "predy = logistic.predict(validx)\n",
    "print('evaluation for logistic')\n",
    "evaluate(validy, predy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from keras.layers import Dropout, Reshape, Conv2D, MaxPooling2D, Maximum, LocallyConnected2D\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "\n",
    "nets = {}\n",
    "\n",
    "nets['google'] = Sequential([\n",
    "    Reshape([28, 28, 1], input_shape=(trainx.shape[1],)),\n",
    "    Conv2D(32, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    Conv2D(64, [5, 5], strides=(1, 1), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    Reshape([28 * 28 * 64 // 4 // 4]),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "nets['mlp-6'] = Sequential([\n",
    "    Dense(600, activation='relu', input_dim=trainx.shape[1]),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(600, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "my_input = Input(shape=(trainx.shape[1],))\n",
    "net = Reshape([28, 28, 1])(my_input)\n",
    "net = Conv2D(32, [5, 5], strides=(1, 1), padding='same', activation='relu')(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2), padding='same')(net)\n",
    "parallel_lcs = [\n",
    "    LocallyConnected2D(16, [5, 5], strides=(2, 2), padding='valid', activation='relu')(net)\n",
    "    for _ in range(16)]\n",
    "net = Maximum()(parallel_lcs)\n",
    "net = Reshape([net.shape[1:].num_elements()])(net)\n",
    "net = Dropout(0.8)(net)\n",
    "net = Dense(10, activation='softmax')(net)\n",
    "net = Model(inputs=my_input, outputs=net)\n",
    "nets['my-cnn'] = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "Training new model: my-cnn\n",
      "*********************************************\n",
      "compiling net...   0 sec\n",
      "moving ../models/mnist/my-cnn-best.hdf5 to ../models/mnist/my-cnn-best.hdf5-old\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 216s - loss: 0.5629 - acc: 0.8221 - val_loss: 0.1376 - val_acc: 0.9606\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.2290 - acc: 0.9318 - val_loss: 0.0877 - val_acc: 0.9731\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.1756 - acc: 0.9479 - val_loss: 0.0685 - val_acc: 0.9793\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.1441 - acc: 0.9568 - val_loss: 0.0592 - val_acc: 0.9824\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.1327 - acc: 0.9610 - val_loss: 0.0527 - val_acc: 0.9838\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.1172 - acc: 0.9647 - val_loss: 0.0480 - val_acc: 0.9849\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.1113 - acc: 0.9668 - val_loss: 0.0444 - val_acc: 0.9862\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.1030 - acc: 0.9689 - val_loss: 0.0420 - val_acc: 0.9867\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.0925 - acc: 0.9712 - val_loss: 0.0386 - val_acc: 0.9877\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.0932 - acc: 0.9725 - val_loss: 0.0387 - val_acc: 0.9879\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.0860 - acc: 0.9730 - val_loss: 0.0369 - val_acc: 0.9888\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 199s - loss: 0.0832 - acc: 0.9741 - val_loss: 0.0352 - val_acc: 0.9887\n",
      " 2412 sec\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ../models && mkdir -p ../models/mnist\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def rectime(name='', fmt='{: 4.0f}'):\n",
    "    print(name, end='')\n",
    "    sys.stdout.flush()\n",
    "    t = time.time()\n",
    "    yield\n",
    "    t = time.time() - t\n",
    "    print(fmt.format(t), 'sec')\n",
    "    \n",
    "save_files = {name: '../models/mnist/' + name + '-best.hdf5' for name in nets.keys()}\n",
    "for net_name, net in nets.items():\n",
    "    if not net_name.startswith('my-cnn'):\n",
    "        continue\n",
    "    print('*********************************************')\n",
    "    print('Training new model:', net_name)\n",
    "    print('*********************************************')\n",
    "\n",
    "    \n",
    "    with rectime(name='compiling net...'):\n",
    "        net.compile(\n",
    "            loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    f = save_files[net_name]\n",
    "    ! if [ -f {f} ] ; then echo moving {f} to {f}-old; mv {f} {f}-old; fi\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc', min_delta=0, patience=0)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(f, monitor='val_loss', save_best_only=True)\n",
    "    # TODO more templates to have: TensorBoard on aws; filters (logistic?)\n",
    "    # feeder queues\n",
    "    # norms over gradients (tensorboard example?)\n",
    "    with rectime():\n",
    "        net.fit(\n",
    "            trainx,\n",
    "            trainy,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "            validation_data=(validx, validy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17952/18000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028317046098501629, 0.99127777777777781]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nets['google'].load_weights(save_files['google'])\n",
    "nets['google'].evaluate(validx, validy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 2310.00 556.00\" width=\"2310pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-552 2306,-552 2306,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140488736437472 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140488736437472</title>\n",
       "<polygon fill=\"none\" points=\"1085,-511.5 1085,-547.5 1217,-547.5 1217,-511.5 1085,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-525.8\">input_42: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140488736004584 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140488736004584</title>\n",
       "<polygon fill=\"none\" points=\"1082,-438.5 1082,-474.5 1220,-474.5 1220,-438.5 1082,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-452.8\">reshape_136: Reshape</text>\n",
       "</g>\n",
       "<!-- 140488736437472&#45;&gt;140488736004584 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140488736437472-&gt;140488736004584</title>\n",
       "<path d=\"M1151,-511.313C1151,-503.289 1151,-493.547 1151,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1154.5,-484.529 1151,-474.529 1147.5,-484.529 1154.5,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488736086056 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140488736086056</title>\n",
       "<polygon fill=\"none\" points=\"1082.5,-365.5 1082.5,-401.5 1219.5,-401.5 1219.5,-365.5 1082.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-379.8\">conv2d_244: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140488736004584&#45;&gt;140488736086056 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140488736004584-&gt;140488736086056</title>\n",
       "<path d=\"M1151,-438.313C1151,-430.289 1151,-420.547 1151,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1154.5,-411.529 1151,-401.529 1147.5,-411.529 1154.5,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488736005256 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140488736005256</title>\n",
       "<polygon fill=\"none\" points=\"1044,-292.5 1044,-328.5 1258,-328.5 1258,-292.5 1044,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-306.8\">max_pooling2d_92: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140488736086056&#45;&gt;140488736005256 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140488736086056-&gt;140488736005256</title>\n",
       "<path d=\"M1151,-365.313C1151,-357.289 1151,-347.547 1151,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1154.5,-338.529 1151,-328.529 1147.5,-338.529 1154.5,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488736187280 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140488736187280</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 272,-255.5 272,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"136\" y=\"-233.8\">locally_connected2d_43: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488736187280 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140488736005256-&gt;140488736187280</title>\n",
       "<path d=\"M1043.7,-304.931C881.222,-297.56 562.854,-281.349 282.17,-256.077\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"282.477,-252.59 272.202,-255.174 281.845,-259.562 282.477,-252.59\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488735366672 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140488735366672</title>\n",
       "<polygon fill=\"none\" points=\"290,-219.5 290,-255.5 562,-255.5 562,-219.5 290,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-233.8\">locally_connected2d_44: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488735366672 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140488736005256-&gt;140488735366672</title>\n",
       "<path d=\"M1043.75,-300.516C929.543,-290.742 743.234,-274.202 572.404,-256.134\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"572.527,-252.628 562.214,-255.053 571.788,-259.589 572.527,-252.628\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488735446184 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140488735446184</title>\n",
       "<polygon fill=\"none\" points=\"580,-219.5 580,-255.5 852,-255.5 852,-219.5 580,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"716\" y=\"-233.8\">locally_connected2d_45: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488735446184 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140488736005256-&gt;140488735446184</title>\n",
       "<path d=\"M1046.81,-292.494C981.658,-281.86 897.693,-268.156 830.658,-257.214\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"830.906,-253.709 820.473,-255.552 829.779,-260.617 830.906,-253.709\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488734395864 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140488734395864</title>\n",
       "<polygon fill=\"none\" points=\"870,-219.5 870,-255.5 1142,-255.5 1142,-219.5 870,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1006\" y=\"-233.8\">locally_connected2d_46: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488734395864 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140488736005256-&gt;140488734395864</title>\n",
       "<path d=\"M1116.27,-292.494C1096.42,-282.773 1071.32,-270.487 1050.09,-260.09\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1051.47,-256.868 1040.95,-255.614 1048.39,-263.155 1051.47,-256.868\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488734072784 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140488734072784</title>\n",
       "<polygon fill=\"none\" points=\"1160,-219.5 1160,-255.5 1432,-255.5 1432,-219.5 1160,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1296\" y=\"-233.8\">locally_connected2d_47: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488734072784 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140488736005256-&gt;140488734072784</title>\n",
       "<path d=\"M1185.73,-292.494C1205.58,-282.773 1230.68,-270.487 1251.91,-260.09\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1253.61,-263.155 1261.05,-255.614 1250.53,-256.868 1253.61,-263.155\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488733163136 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140488733163136</title>\n",
       "<polygon fill=\"none\" points=\"1450,-219.5 1450,-255.5 1722,-255.5 1722,-219.5 1450,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1586\" y=\"-233.8\">locally_connected2d_48: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488733163136 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140488736005256-&gt;140488733163136</title>\n",
       "<path d=\"M1255.19,-292.494C1320.34,-281.86 1404.31,-268.156 1471.34,-257.214\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1472.22,-260.617 1481.53,-255.552 1471.09,-253.709 1472.22,-260.617\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488732769416 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140488732769416</title>\n",
       "<polygon fill=\"none\" points=\"1740,-219.5 1740,-255.5 2012,-255.5 2012,-219.5 1740,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1876\" y=\"-233.8\">locally_connected2d_49: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488732769416 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140488736005256-&gt;140488732769416</title>\n",
       "<path d=\"M1258.25,-300.516C1372.46,-290.742 1558.77,-274.202 1729.6,-256.134\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1730.21,-259.589 1739.79,-255.053 1729.47,-252.628 1730.21,-259.589\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488731871888 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140488731871888</title>\n",
       "<polygon fill=\"none\" points=\"2030,-219.5 2030,-255.5 2302,-255.5 2302,-219.5 2030,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2166\" y=\"-233.8\">locally_connected2d_50: LocallyConnected2D</text>\n",
       "</g>\n",
       "<!-- 140488736005256&#45;&gt;140488731871888 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140488736005256-&gt;140488731871888</title>\n",
       "<path d=\"M1258.3,-304.931C1420.78,-297.56 1739.15,-281.349 2019.83,-256.077\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2020.15,-259.562 2029.8,-255.174 2019.52,-252.59 2020.15,-259.562\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488735847088 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140488735847088</title>\n",
       "<polygon fill=\"none\" points=\"1071,-146.5 1071,-182.5 1231,-182.5 1231,-146.5 1071,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-160.8\">maximum_38: Maximum</text>\n",
       "</g>\n",
       "<!-- 140488736187280&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140488736187280-&gt;140488735847088</title>\n",
       "<path d=\"M272.202,-219.826C275.16,-219.54 278.096,-219.264 281,-219 565.057,-193.147 902.454,-176.362 1060.66,-169.306\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1060.98,-172.795 1070.82,-168.855 1060.67,-165.802 1060.98,-172.795\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488735366672&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140488735366672-&gt;140488735847088</title>\n",
       "<path d=\"M562.214,-219.947C565.169,-219.625 568.1,-219.309 571,-219 744.01,-200.584 946.456,-182.756 1060.62,-173.044\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1061.04,-176.521 1070.71,-172.187 1060.45,-169.546 1061.04,-176.521\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488735446184&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140488735446184-&gt;140488735847088</title>\n",
       "<path d=\"M820.193,-219.494C893.501,-207.529 990.634,-191.675 1060.66,-180.245\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1061.49,-183.657 1070.79,-178.592 1060.36,-176.748 1061.49,-183.657\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488734395864&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>140488734395864-&gt;140488735847088</title>\n",
       "<path d=\"M1040.73,-219.494C1060.58,-209.773 1085.68,-197.487 1106.91,-187.09\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1108.61,-190.155 1116.05,-182.614 1105.53,-183.868 1108.61,-190.155\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488734072784&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>140488734072784-&gt;140488735847088</title>\n",
       "<path d=\"M1261.27,-219.494C1241.42,-209.773 1216.32,-197.487 1195.09,-187.09\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1196.47,-183.868 1185.95,-182.614 1193.39,-190.155 1196.47,-183.868\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488733163136&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>140488733163136-&gt;140488735847088</title>\n",
       "<path d=\"M1481.81,-219.494C1408.5,-207.529 1311.37,-191.675 1241.34,-180.245\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1241.64,-176.748 1231.21,-178.592 1240.51,-183.657 1241.64,-176.748\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488732769416&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>140488732769416-&gt;140488735847088</title>\n",
       "<path d=\"M1739.79,-219.947C1736.83,-219.625 1733.9,-219.309 1731,-219 1557.99,-200.584 1355.54,-182.756 1241.38,-173.044\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1241.55,-169.546 1231.29,-172.187 1240.96,-176.521 1241.55,-169.546\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488731871888&#45;&gt;140488735847088 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>140488731871888-&gt;140488735847088</title>\n",
       "<path d=\"M2029.8,-219.826C2026.84,-219.54 2023.9,-219.264 2021,-219 1736.94,-193.147 1399.55,-176.362 1241.34,-169.306\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1241.33,-165.802 1231.18,-168.855 1241.02,-172.795 1241.33,-165.802\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488731477664 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140488731477664</title>\n",
       "<polygon fill=\"none\" points=\"1082,-73.5 1082,-109.5 1220,-109.5 1220,-73.5 1082,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-87.8\">reshape_137: Reshape</text>\n",
       "</g>\n",
       "<!-- 140488735847088&#45;&gt;140488731477664 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>140488735847088-&gt;140488731477664</title>\n",
       "<path d=\"M1151,-146.313C1151,-138.289 1151,-128.547 1151,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1154.5,-119.529 1151,-109.529 1147.5,-119.529 1154.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488731478168 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140488731478168</title>\n",
       "<polygon fill=\"none\" points=\"1093,-0.5 1093,-36.5 1209,-36.5 1209,-0.5 1093,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1151\" y=\"-14.8\">dense_261: Dense</text>\n",
       "</g>\n",
       "<!-- 140488731477664&#45;&gt;140488731478168 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>140488731477664-&gt;140488731478168</title>\n",
       "<path d=\"M1151,-73.3129C1151,-65.2895 1151,-55.5475 1151,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1154.5,-46.5288 1151,-36.5288 1147.5,-46.5289 1154.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import graphviz\n",
    "nets2=nets\n",
    "# https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "SVG(model_to_dot(nets['my-cnn3']).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo do this via tensorboard?\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'block5_conv1'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(0, 200):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 8\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "imsave('stitched_filters_%dx%d.png' % (n, n), stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_2/Relu:0' shape=(?, 14, 14, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "tf.image_summary('conv1/filters', kernel_transposed, max_images=3)\n",
    "\n",
    "K.get_session().run(net.layers[4:7][0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopRetrain(keras.callbacks.Callback):\n",
    "    \"\"\"Callback that terminates training when a NaN loss is encountered.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_error):\n",
    "        super(EarlyStopRetrain, self).__init__()\n",
    "        self.stop_error = stop_error\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            for k in self.params['metrics']:\n",
    "                if k in self.totals:\n",
    "                    # Make value available to next callbacks.\n",
    "                    logs[k] = self.totals[k] / self.seen\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print('Batch %d: Invalid loss, terminating training' % (batch))\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate architectures (but still scoped)\n",
    "# and a special version of LeNet I call what-I-vaguely-remember-of-LeNet-net.\n",
    "\n",
    "# NO MORE TENSORLAYER - TF-slim.\n",
    "\n",
    "# TODO smarter init\n",
    "# TODO class balancing?\n",
    "#http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/utils.html#class_balancing_oversample\n",
    "def mlp_drop3(input_layer, is_train=True):\n",
    "    with tf.variable_scope('mlp-dropout-3', reuse=True):\n",
    "        net = tl.layers.DropoutLayer(input_layer, 0.8, True, is_train, name='drop_input')\n",
    "        net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='dense1')\n",
    "        net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop1')\n",
    "        net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='dense2')\n",
    "        net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop2')\n",
    "        net = tl.layers.DenseLayer(net, n_units=10, act=tf.identity, name='dense3')\n",
    "        return net\n",
    "\n",
    "def wrap_net(network_generator, is_train=True, batch_size=None):\n",
    "    x = tf.placeholder(tf.float32, shape=[batch_size, 784], name='x')\n",
    "    y = tf.placeholder(tf.int64, shape=[batch_size], name='y')\n",
    "    net = network_generator(tl.layers.InputLayer(x, name='input_layer3'), is_train=is_train)\n",
    "    cost = tl.cost.cross_entropy(net.outputs, y, name='cost')\n",
    "    output = tf.argmax(tf.nn.softmax(net.outputs, name='output'), 1)\n",
    "    acc = tf.reduce_mean(output == y)\n",
    "    return net, cost, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_drop_graph = tf.Graph()\n",
    "\n",
    "with mlp_drop_graph.as_default():\n",
    "    net, cost, acc = wrap_net(mlp_drop3, is_train=True, batch_size=128)\n",
    "\n",
    "    params = network.all_params\n",
    "    train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate=0.001, beta1=0.9, beta2=0.99, epsilon=1e-8, use_locking=False).minimize(cost)\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "net.print_params()\n",
    "net.print_layers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/utils.html#fit and below\n",
    "# save to ../logs. Models can go in ../checkpoints (date and epoch count to delineate, too.)\n",
    "for epoch in range(10):\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(\n",
    "        X_train, y_train, batch_size, shuffle=True):\n",
    "            feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "            feed_dict.update(network.all_drop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO momentum cooling\n",
    "global_step = tf.Variable(\n",
    "    0, trainable=False)  # don't train a global step value\n",
    "initial_learning_rate, decay_fraction, decay_period = 0.01, 0.95, 10000\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    initial_learning_rate,\n",
    "    global_step,\n",
    "    decay_period,\n",
    "    decay_fraction,\n",
    "    staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "             .minimize(sm.cross_entropy, global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_FOLDS = 4\n",
    "UPDATE_COARSENESS = 5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "top_models = []\n",
    "\n",
    "# Cross validate on random restarts\n",
    "for fold, validate in enumerate(train.cross_validation(NUM_FOLDS), 1):\n",
    "    print('Starting fold {}'.format(fold))\n",
    "    sess.run(tf.initialize_all_variables())  # random restart\n",
    "    best_in_fold_validation = np.inf\n",
    "    best_in_fold_epoch = None\n",
    "    best_in_fold_model = None\n",
    "\n",
    "    for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "        for batch in train.new_epoch(BATCH_SIZE):\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1]}, session=sess)\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            valid_err = validate.multiclass_error(x, sm.y, y, session=sess)\n",
    "            print('  Epoch {:3} error {}'.format(epoch, valid_err))\n",
    "            if valid_err > best_in_fold_validation: break\n",
    "            best_in_fold_validation = valid_err\n",
    "            best_in_fold_epoch = epoch\n",
    "            best_in_fold_model = saver.save(\n",
    "                sess, '/tmp/tf-mnist-batch-sgd-fold-{}.ckpt'.format(fold))\n",
    "    top_models.append((fold, best_in_fold_epoch, best_in_fold_model))\n",
    "\n",
    "# We select the model based on the entire training data set rather than the error from the\n",
    "# cross-validation fold because that error was used for early stopping. It's unfair to\n",
    "# compare across the folds the same accuracy since some folds might be easier than others.\n",
    "# Doing this helps the test accuracy by about 0.5%.\n",
    "best_fold, best_epoch, best_model = None, None, None\n",
    "best_error = np.inf\n",
    "for fold, epoch, model in top_models:\n",
    "    saver.restore(sess, model)\n",
    "    error = train.multiclass_error(x, sm.y, y, session=sess)\n",
    "    if best_error > error:\n",
    "        best_error, best_fold, best_epoch, best_model = error, fold, epoch, model\n",
    "\n",
    "print('Fold {}/{} epoch {}/{} with inside training dataset-error {}'\n",
    "      .format(best_fold, NUM_FOLDS, best_epoch, NUM_EPOCHS, best_error))\n",
    "\n",
    "saver.restore(sess, best_model)\n",
    "\n",
    "print('Test error', test.multiclass_error(x, sm.y, y, session=sess))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_prefix = 'mnist-cnn-'\n",
    "cnn_suffix = '.ckpt'\n",
    "available_epochs = [x for x in os.listdir('../data')\n",
    "                    if x.startswith(cnn_prefix) and x.endswith(cnn_suffix)]\n",
    "print(available_epochs)\n",
    "\n",
    "def extract_epoch_number(s):\n",
    "    return int(s[len(cnn_prefix):-len(cnn_suffix)])\n",
    "\n",
    "available_epochs = [extract_epoch_number(s) for s in available_epochs]\n",
    "max_epoch = max(available_epochs or [None])\n",
    "print('{} saved epoch files; using largest epoch ({}) as start'\n",
    "      .format(len(available_epochs), max_epoch))\n",
    "\n",
    "def epoch_to_filename(e):\n",
    "    return '../data/' + cnn_prefix + str(e) + cnn_suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) # any batch size on flattened pixel values\n",
    "y = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Don't drop out when testing by setting keep_prob to 1.0\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if max_epoch:\n",
    "    saver.restore(sess, epoch_to_filename(max_epoch))\n",
    "    print('Restoring CNN from epoch {}'.format(max_epoch))\n",
    "else:\n",
    "    print('Starting CNN training from scratch')\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    max_epoch = 0\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 35\n",
    "UPDATE_COARSENESS = 5\n",
    "for epoch in range(max_epoch + 1, NUM_EPOCHS + 1):\n",
    "    for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "        tot_batches = train.size // BATCH_SIZE\n",
    "        two_percent_done = i * 50 // tot_batches\n",
    "        print(('\\rEpoch {}/{} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "               + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5}, session=sess)\n",
    "\n",
    "    print(']')\n",
    "    name = saver.save(sess, epoch_to_filename(epoch))\n",
    "    print('  Saved to', name)\n",
    "    if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "        err = train.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)\n",
    "        print('  Epoch {}/{} training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "\n",
    "print('Test error {}'.format(test.multiclass_error(\n",
    "            x, output.y, y, feed_dict={keep_prob:1.0}, session=sess)))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
